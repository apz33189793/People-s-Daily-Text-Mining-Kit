{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6172d3e0-3bc6-430a-b729-db7a104aac61",
   "metadata": {
    "id": "6172d3e0-3bc6-430a-b729-db7a104aac61"
   },
   "source": [
    "# function_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6959b-f93f-445c-848d-75847e72b9b0",
   "metadata": {
    "id": "97b6959b-f93f-445c-848d-75847e72b9b0"
   },
   "outputs": [],
   "source": [
    "def function_list():\n",
    "    doc=\"\"\"\n",
    "    基本概況\n",
    "        dfi=basic_info(year, path, title,rolling=1)\n",
    "        df_kw= people.key_word_trend_all_year(year, kw, path, title,rolling=1)   \n",
    "        df_kt= people.key_word_trend(df, kw, field, time, pth, title,rolling=1)\n",
    "    資料選取\n",
    "        (df, df_year)= people.doc_select(year, kw, field, unit, path, title)\n",
    "        people.print_doc_select_random(df,num,field)\n",
    "        df_ks=people.select_document_by_key_word(df, field, kw ,num, pth)\n",
    "        people.plot_doc_select_year(df_year)\n",
    "        people.print_filename(filename, field, path)\n",
    "    詞頻與詞對\n",
    "        df_corpus= people.corpus_map_year(df,kw, corpus,window, pth,title)\n",
    "        df_corpus= people.read_corpus_map_year(kw,pth)\n",
    "    情感分析\n",
    "        df_pn= people.sentiment_analysis(df, field, path, title,rolling=1)\n",
    "        df_pn= people.read_sentiment_cluster_analysis(pth,title)\n",
    "    主題分析\n",
    "        (df_tp, df_topic,df_lsa)= people.topic_model(df, field,num_i, method, pth, title)  \n",
    "        (df_tp, df_topic,df_lsa)= people.read_topic_model(pth, title) \n",
    "        ts= people.plot_topic_model(df_lsa,labels,rolling=1) \n",
    "    文章分群\n",
    "        (df_cl, clus_token, df_rate, df_count, df_clus_most)= people.cluster_analysis(df, field,pth,title, method='svd', num_i=100, n_clus=10)\n",
    "        (df_cl, clus_token, df_rate, df_count, df_clus_most)= people.read_cluster_analysis(pth,title)\n",
    "        (df_count, df_year)= people.plot_cluster_analysis(df, labels,pth, rolling=1)\n",
    "    潛在語意\n",
    "        (df_concept, transformer, lsa, term_list)= people.latent_sementic( df, field, num_i,  pth=,title)\n",
    "        closest= people.kClosestTerms(k,term,transformer,lsa,term_list)\n",
    "        simi= people.similarity(a,b)\n",
    "    繪圖\n",
    "        people.word_cloud_map(words,title, mask,pth)\n",
    "        df_var = people.plot_variables_compare(df, variables)\n",
    "        df_cat = people.plot_single_variable_cate(df, variable,cate)\n",
    "        df_mean = people.plot_variables_year_trend(df, variables,rolling=1)\n",
    "    斷詞\n",
    "        df= people.jieba_token(df,field,pth, title)\n",
    "        df= people.read_jieba_token(pth, title)    \n",
    "    其他\n",
    "        df=people.read_file(filename,pth)\n",
    "        people.save_file(df,filename, pth)       \n",
    "        people.note()\n",
    "        people.function_list()\n",
    "    \"\"\"\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd88468-b1de-4713-8205-d1fd4585bfc6",
   "metadata": {
    "id": "3cd88468-b1de-4713-8205-d1fd4585bfc6"
   },
   "outputs": [],
   "source": [
    "def note():\n",
    "    doc=\"\"\"\n",
    "    檔案結構\n",
    "        path=\"/content/drive/MyDrive/colab/people/\"  檔案讀取路徑\n",
    "        pth=\"/content/drive/my python/colab/people/\" 檔案貯存路徑 \n",
    "        colab/people/jieba/ 斷好詞的結巴檔，一年一個檔\n",
    "        colab/people/people/data/  程式會用到的資料檔\n",
    "        colab/people/people/corpus/ 程式會用到的語料，如 userdic.txt\n",
    "        colab/people/people/pic/  程式會用到的程式檔，如 china.jpg\n",
    "        colab/people/people/plot/ 分析之後產生的圖檔\n",
    "    使用方法\n",
    "        import people 叫入模組，顯示函數列表與指令範例\n",
    "        直接拷貝範例指令，調整參數，即可執行。\n",
    "    修改模組\n",
    "        編修 people.ipynb\n",
    "        存成可執行檔 people.py\n",
    "        重新與google drtive 連線才會生效。\n",
    "    \"\"\"\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae0b84-e431-417f-a9d6-b3e58c3b9ab9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfae0b84-e431-417f-a9d6-b3e58c3b9ab9",
    "outputId": "e6adad21-5808-4a73-9902-4c10fe6edfc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    基本概況\n",
      "        dfi=basic_info(year, path, title,rolling=1)\n",
      "        df_kw= people.key_word_trend_all_year(year, kw, path, title,rolling=1)   \n",
      "        df_kt= people.key_word_trend(df, kw, field, time, pth, title,rolling=1)\n",
      "    資料選取\n",
      "        (df, df_year)= people.doc_select(year, kw, field, unit, path, title)\n",
      "        df= people.read_doc_select(unit, field, pth, title) \n",
      "        df_year= people.read_doc_select_year(unit, field, pth, title) \n",
      "        people.print_doc_select_random(df,num,field)\n",
      "        df_ks=people.select_document_by_key_word(df, field, kw ,num, pth)\n",
      "        people.plot_doc_select_year(df_year)\n",
      "        people.print_filename(filename, field, path)\n",
      "    詞頻與詞對\n",
      "        df_corpus= people.corpus_map_year(df,kw, corpus,window, pth,title)\n",
      "        df_corpus= people.read_corpus_map_year(kw,pth)\n",
      "    情感分析\n",
      "        df_pn= people.sentiment_analysis(df, field, path, title,rolling=1)\n",
      "        df_pn= people.read_sentiment_cluster_analysis(pth,title)\n",
      "    主題分析\n",
      "        (df_tp, df_topic,df_lsa)= people.topic_model(df,field,num_i=10, method,pth, title)  \n",
      "        (df_tp, df_topic,df_lsa)= people.read_topic_model(pth, title) \n",
      "        ts= people.plot_topic_model(df_lsa,labels,rolling=1) \n",
      "    文章分群\n",
      "        (df_cl, clus_token, df_rate, df_count, df_clus_most)= people.cluster_analysis(df, field,pth,title, method='svd', num_i=100, n_clus=10)\n",
      "        (df_cl, clus_token, df_rate, df_count, df_clus_most)= people.read_cluster_analysis(pth,title)\n",
      "        (df_count, df_year)= people.plot_cluster_analysis(df, labels,pth, rolling=1)\n",
      "    潛在語意\n",
      "        (df_concept, transformer, lsa, term_list)= people.lsa_concept(df, field, num_i, pth,title)\n",
      "        closest= people.kClosestTerms(k,term,transformer,lsa,term_list)\n",
      "        simi= people.similarity(a,b)\n",
      "    繪圖\n",
      "        people.word_cloud_map(words,title, mask,pth)\n",
      "        df_var = people.plot_variables_compare(df, variables)\n",
      "        df_cat = people.plot_single_variable_cate(df, variable,cate)\n",
      "        df_mean = people.plot_variables_year_trend(df, variables,rolling=1)\n",
      "    斷詞\n",
      "        df= people.jieba_token(df,field,pth, title)\n",
      "        df= people.read_jieba_token(pth, title)    \n",
      "    其他\n",
      "        df=people.read_excel_file(ilename,pth)\n",
      "        df=people.read_csv_file(ilename,pth)        \n",
      "        df=people.read_pkl_file(filename, pth)\n",
      "        people.save_excel_file(df,filename, pth)       \n",
      "        people.save_csv_file(df,filename, pth)  \n",
      "        people.save_pkl_file(df,filename, pth) \n",
      "        people.note()\n",
      "        people.function_list()\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "function_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SNm40o7FgSmF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNm40o7FgSmF",
    "outputId": "605e8dcc-87c3-4897-b80c-76c510937c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    檔案結構\n",
      "        path=\"/content/drive/MyDrive/colab/people/\"  檔案讀取路徑\n",
      "        pth=\"/content/drive/my python/colab/people/\" 檔案貯存路徑 \n",
      "        colab/people/jieba/ 斷好詞的結巴檔，一年一個檔\n",
      "        colab/people/people/data/  程式會用到的資料檔\n",
      "        colab/people/people/corpus/ 程式會用到的語料，如 userdic.txt\n",
      "        colab/people/people/pic/  程式會用到的程式檔，如 china.jpg\n",
      "        colab/people/people/plot/ 分析之後產生的圖檔\n",
      "    使用方法\n",
      "        import people 叫入模組，顯示函數列表與指令範例\n",
      "        直接拷貝範例指令，調整參數，即可執行。\n",
      "    修改模組\n",
      "        編修 people.ipynb\n",
      "        存成可執行檔 people.py\n",
      "        重新與google drtive 連線才會生效。\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "note()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef306014-712f-4012-9de0-a76ec4637e02",
   "metadata": {
    "id": "ef306014-712f-4012-9de0-a76ec4637e02"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, pickle, re, os, io, random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"font.sans-serif\"] = [\"kaiu\"] \n",
    "rcParams[\"font.family\"] =\"DFKai-sb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ZZ4PaAdvt5W6",
   "metadata": {
    "id": "ZZ4PaAdvt5W6"
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF,LatentDirichletAllocation,TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nFcFDo0GZmp5",
   "metadata": {
    "id": "nFcFDo0GZmp5"
   },
   "source": [
    "# 基本概況"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "teHC5485346c",
   "metadata": {
    "id": "teHC5485346c"
   },
   "source": [
    "## basic_info(year, path)\n",
    "* 統計篇數、段落數、句數、詞數、字數，繪製趨勢圖比較\n",
    "* path 為讀檔之路徑， path=\"/content/drive/MyDrive/colab/people/\" 或 D:/my python/people/\n",
    "* 結果檔存於 pth+\"data/\"+ \"_basic_count_year.xlsx\"\n",
    "* pth 為存檔之路徑 pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Tm3sb_cQ4bY6",
   "metadata": {
    "id": "Tm3sb_cQ4bY6"
   },
   "outputs": [],
   "source": [
    "def basic_info(year, path,title):\n",
    "    years=[]\n",
    "    count_arti=[]\n",
    "    count_para=[]\n",
    "    count_sent=[]\n",
    "    count_token=[]\n",
    "    count_word=[]\n",
    "    for n in range(len(year)):\n",
    "        fname=str(year[n])+\"_jieba.xlsx\"\n",
    "        df=pd.read_excel(path+\"jieba/\"+fname, 0)\n",
    "        con=df[\"doc_content\"].str.replace(\"    \",\"\\n\")\n",
    "        con_list=list(con)  \n",
    "        token=df[\"token\"]\n",
    "        token_list=list(con)         \n",
    "        # 文章數\n",
    "        count_arti.append(len(con_list))\n",
    "        c_para=0\n",
    "        c_sent=0\n",
    "        c_token=0\n",
    "        c_word=0\n",
    "        for t in range(len(con_list)):\n",
    "            if type(con_list[t])== str:\n",
    "                # 段落\n",
    "                obj=re.split('[\\n]',con_list[t])  \n",
    "                c_para+=len(obj)\n",
    "                # 句子        \n",
    "                obj=re.split('[？！：。\\n]',con_list[t])\n",
    "                c_sent+=len(obj)            \n",
    "                # 字詞      \n",
    "                obj=token_list[t].split()\n",
    "                c_token+=len(obj)  \n",
    "                # 字    \n",
    "                obj=con_list[t].replace(' ','')\n",
    "                c_word+=len(obj)  \n",
    "        count_para.append(c_para)     \n",
    "        count_sent.append(c_sent) \n",
    "        count_token.append(c_token) \n",
    "        count_word.append(c_word) \n",
    "        years.append(year[n])\n",
    "        print (year[n],\"done\",end=\" \")   \n",
    "    # dataframe    \n",
    "    data=list(zip(years,count_arti, count_para, count_sent, count_token, count_word))\n",
    "    df=pd.DataFrame(data,columns=[\"year\",\"count_arti\", \"count_para\", \"count_sent\", \"count_token\", \"count_word\"])\n",
    "    # plot   \n",
    "    plt.figure(figsize=(20,10))\n",
    "    df.index=year\n",
    "    df[[\"count_arti\",\"count_para\"]].plot(figsize=(20,10))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    df.index=year\n",
    "    df[[\"count_sent\",\"count_token\",\"count_word\"]].plot(figsize=(20,10))\n",
    "    plt.show()\n",
    "\n",
    "    # save\n",
    "    title='count_arti_para_sent_token_word'\n",
    "    if path.startswith('/content'):\n",
    "        pth=\"/content/drive/MyDrive/my python/people/\"\n",
    "    else:\n",
    "        pth= \"D:/my python/people/\"\n",
    "    fname=pth+\"data/\"+title+\".xlsx\"\n",
    "    writer = pd.ExcelWriter(fname)\n",
    "    df.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "    writer.save() \n",
    "    return df  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZojaiadjaSvI",
   "metadata": {
    "id": "ZojaiadjaSvI"
   },
   "source": [
    "## key_word_trend_all_year(year, w,  path, title,rolling)\n",
    "* 由關鍵字的數量觀察文本的的變化趨勢，以月為單位加總，內定為全部年度 1946 至今。繪製折線圖。資料來源為 jieba 資料夾內的 excel 檔，欄位為 doc_content\n",
    "* kw 為 list of list, 不同的字詞組之間的比較。\n",
    "* title 標題\n",
    "* rolling 為平滑化的單位，取前面n 個數值平均\n",
    "* path 為讀檔之路徑， path=\"/content/drive/MyDrive/colab/people/\" 或 D:/my python/people/\n",
    "* 結果檔存於 pth+\"data/\"+ title + \"_key_word_trend_year.xlsx\"\n",
    "* pth 為存檔之路徑 pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/\n",
    "* step=1 為簡化測式，step 代表每隔幾筆選取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SoVhlTkdaSvI",
   "metadata": {
    "id": "SoVhlTkdaSvI"
   },
   "outputs": [],
   "source": [
    "def key_word_trend_all_year(year, kw, path, title,rolling=1):\n",
    "        # 空白的dataframe\n",
    "        df2=pd.DataFrame()\n",
    "        # 逐篇文章計算  \n",
    "        for n in range(len(year)):\n",
    "            # 讀取 xlsx 成為 dataframe\n",
    "            path2= path+ 'jieba/'\n",
    "            fname=str(year[n])+\"_jieba.xlsx\"\n",
    "            df=pd.read_excel(path2+fname, 0)   \n",
    "            # 選擇欄位 doc_content\n",
    "            content=list(df.doc_content)\n",
    "            # 年月\n",
    "            era_list=list(df.era.unique())\n",
    "            era_list=sorted(era_list)\n",
    "            count_y=[]\n",
    "            ym=[]\n",
    "            for m in range(len(era_list)):\n",
    "                df_ym=df[df['era']==era_list[m]]\n",
    "                # 各月資料\n",
    "                count_m=[era_list[m]]\n",
    "                # 關鍵字組\n",
    "                for k in kw:\n",
    "                    num=0\n",
    "                    total=0\n",
    "                    # 單一關鍵字\n",
    "                    for j in k:\n",
    "                        num+=sum(df_ym.doc_content.str.count(j))  \n",
    "                        total+=sum(df_ym.doc_content.str.len())\n",
    "                    rate=num/total           \n",
    "                    count_m.append(num)\n",
    "                    count_m.append(rate)\n",
    "                count_y.append(count_m)          \n",
    "            # 垂直合併    \n",
    "            df0=pd.DataFrame(count_y)\n",
    "            df2=df2.append(df0)  \n",
    "            # 計數\n",
    "            print (year[n], end=\",\")\n",
    "\n",
    "        # 繪圖，記得我們已先設定了字型\n",
    "        df2['date'] = pd.to_datetime(df2[0], format=\"%Y%m\")\n",
    "        df2.set_index('date', inplace=True)\n",
    "        # 圖面大小\n",
    "        plt.figure(figsize=(16,9))\n",
    "        for k in range(2,len(df2.columns),2):\n",
    "            plt.plot(df2[k].rolling(rolling).mean(),label=\",\".join(kw[int(k/2-1)]))\n",
    "        # 圖例\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  \n",
    "        plt.show()\n",
    "        return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xcQoVuz9aSvJ",
   "metadata": {
    "id": "xcQoVuz9aSvJ"
   },
   "source": [
    "## key_word_trend(df, kw, field, time,  pth, title,rolling)\n",
    "* 關鍵字的數量的變化趨勢，繪製折線圖。\n",
    "* df 為 經整理過的 dataframe 檔案。\n",
    "* kw 為 list of list, 不同的 key words 之間的比較。\n",
    "* field 為文本欄位，例如 doc_content\n",
    "* time 為顯示趨勢變化分析單位的欄位，例如 era (年月)\n",
    "* title 為標題\n",
    "* 存檔至 pth/data/ 次目錄 檔名 \"data/\"+ title + \"_key_word_trend.xlsx\"\n",
    "* pth=\"/content/drive/MyDrive/my python/people/或 D:/my python/people/\n",
    "* rolling 為平滑化的單位，取前面n 個數值平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "kn7pYLApaSvJ",
   "metadata": {
    "id": "kn7pYLApaSvJ"
   },
   "outputs": [],
   "source": [
    "def key_word_trend(df,kw, field, time,pth, title, rolling=1):\n",
    "    # 空白的dataframe\n",
    "    df2=pd.DataFrame()\n",
    "\n",
    "    # 選擇欄位 doc_content\n",
    "    content=list(df.doc_content)\n",
    "    era_list=list(df[time].unique())\n",
    "    era_list=sorted(era_list)\n",
    "    count_y=[]\n",
    "    ym=[]\n",
    "    for m in range(len(era_list)):\n",
    "        df_ym=df[df['era']==era_list[m]]\n",
    "        count_m=[era_list[m]]\n",
    "        # 關鍵字組\n",
    "        for k in kw:\n",
    "            num=0\n",
    "            total=0\n",
    "            # 單一關鍵字\n",
    "            for j in k:\n",
    "                num+=sum(df_ym.doc_content.str.count(j))  \n",
    "                total+=sum(df_ym.doc_content.str.len())\n",
    "            rate=num/total           \n",
    "            count_m.append(num)\n",
    "            count_m.append(rate)\n",
    "        count_y.append(count_m)    \n",
    "        \n",
    "    df2.columns=['ym','kw0','rate0','kw1','rate1']\n",
    "        \n",
    "        \n",
    "    # 繪圖，記得我們已先設定了字型\n",
    "    df2['date'] = pd.to_datetime(df2['ym'], format=\"%Y%m\")\n",
    "    df2.set_index('date', inplace=True)\n",
    "\n",
    "    # 圖面大小\n",
    "    plt.figure(figsize=(16,9))\n",
    "    for k in range(len(kw)):\n",
    "        plt.plot(df2[\"rate\"+str(k)],label=\",\".join(kw[k])) \n",
    "    # 圖例\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # 設定時間格式\n",
    "    if len(df3[time+str(0)][0])==4:\n",
    "        df3['date'] = pd.to_datetime(df3[time+str(0)], format=\"%Y\")\n",
    "    elif len(df3[time+str(0)][0])==6:  \n",
    "        df3['date'] = pd.to_datetime(df3[time+str(0)], format=\"%Y%m\")\n",
    "    elif len(df3[time+str(0)][0])==8:  \n",
    "        df3['date'] = pd.to_datetime(df3[time+str(0)], format=\"%Y%m%d\")\n",
    "\n",
    "    df3.set_index('date', inplace=True)\n",
    "    # 折線圖\n",
    "    plt.figure(figsize=(16,10))\n",
    "    for k in range(len(kw)):\n",
    "        plt.plot(df3[\"ratio\"+str(k)].rolling(rolling).mean(),label=\",\".join(kw[k])) \n",
    "    # 圖例，電腦選取位置\n",
    "    plt.legend(loc=0)  \n",
    "    plt.show()  \n",
    "    \n",
    "    # save\n",
    "    fname=pth+\"data/\"+ title + \"_key_word_trend.xlsx\"\n",
    "    writer = pd.ExcelWriter(fname)\n",
    "    df3.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "    writer.save()\n",
    "    return df3   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19defb-afaf-4dab-8fe9-eac47fcaebd1",
   "metadata": {
    "id": "1e19defb-afaf-4dab-8fe9-eac47fcaebd1",
    "tags": []
   },
   "source": [
    "# jieba 資料選取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193dceb-2045-4843-8325-87649a2c7fbc",
   "metadata": {
    "id": "5193dceb-2045-4843-8325-87649a2c7fbc"
   },
   "source": [
    "## doc_select(year, kw, field, unit, path, title)\n",
    "* 從 jieba 資料夾下讀取並篩選資料\n",
    "* year 年度 list\n",
    "* kw 關鍵字 list，token 欄位含有kw 的文本方讀取。\n",
    "* field 選取欄位 token, doc_content, tfidf, textrank,title,author,compilation_name(來源),compilation_vol（版次）,pos(詞性) 選其一\n",
    "* unit 分析單位 sent, para, arti 三選一，如果 field 是 tfidf, textrank, 則 level 為 arti\n",
    "* path 為讀檔之路徑 path=\"/content/drive/MyDrive/colab/people/\" 或 D:/my python/people/\n",
    "* title 為標題。\n",
    "* 自動存結果檔於 pth+\"data/\"+unit+\"_\"+field+\"_\"+title+\".xlsx\"，如果文章的數目太多，超過 excel 之極限，自動改為貯存 pickle 檔。\n",
    "* pth 為存檔之路徑 pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/\n",
    "* step=1 為簡化測式，step 代表每隔幾筆選取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ba75f-77c0-4a17-8acc-9223435dc252",
   "metadata": {
    "id": "801ba75f-77c0-4a17-8acc-9223435dc252"
   },
   "outputs": [],
   "source": [
    "def doc_select(year, kw, field, unit, path, title):\n",
    "    objects=[]\n",
    "    years=[]\n",
    "    filenames=[]\n",
    "    titles=[]\n",
    "    count=[]\n",
    "    rate=[]\n",
    "    ye=[]\n",
    "    for n in range(len(year)):\n",
    "        fname=str(year[n])+\"_jieba.xlsx\"\n",
    "        df=pd.read_excel(path+\"jieba/\"+fname, 0)\n",
    "        con=df[field].str.replace(\"    \",\"\\n\")\n",
    "        con_list=list(con)  \n",
    "        # 分析單元：文章，段落，句子，前後字詞\n",
    "        num_obj=0\n",
    "        for t in range(len(con_list)):\n",
    "            if type(con_list[t])== str:\n",
    "                # 如果 field 是 tfidf or textrank, 改為 arti\n",
    "                if field in [\"tfidf\",\"textrank\"]:\n",
    "                    unit=\"arti\"\n",
    "                    obj=[con_list[t]]\n",
    "                if unit==\"para\":\n",
    "                    # 如果是段落\n",
    "                    obj=re.split('[\\n]',con_list[t])   \n",
    "                    num_obj+=len(obj)  \n",
    "                elif unit==\"sent\":\n",
    "                    # 如果是句子        \n",
    "                    obj=re.split('[？！：。\\n]',con_list[t])\n",
    "                    num_obj+=len(obj) \n",
    "                elif unit==\"arti\":    \n",
    "                    # 以文章為單位\n",
    "                    obj=[con_list[t]]\n",
    "                    num_obj+=len(obj) \n",
    "                # 選擇列入\n",
    "                if len(obj)>0:\n",
    "                    for s in obj:\n",
    "                        # 有關鍵字組\n",
    "                        if len(kw)>0:\n",
    "                            for k in kw:\n",
    "                                if s.find(k)>0:          \n",
    "                                    objects.append(s)\n",
    "                                    years.append(year[n])\n",
    "                                    titles.append(df.title[t])\n",
    "                                    filenames.append(df.filename[t])                            \n",
    "                                    break  \n",
    "                        # 無關鍵字組            \n",
    "                        else:\n",
    "                            objects.append(s)\n",
    "                            years.append(year[n])\n",
    "                            titles.append(df.title[t])\n",
    "                            filenames.append(df.filename[t])   \n",
    "        # 以年為單位，累計次數 \n",
    "        count.append(len(objects))\n",
    "        rate.append(len(objects)/num_obj)\n",
    "        ye.append(year[n])\n",
    "        if unit ==\"arti\":\n",
    "            print (year[n],\"done\", len(objects))\n",
    "        else:    \n",
    "            print (year[n],\"done\",len(objects),num_obj,len(objects)/num_obj)\n",
    "\n",
    "    if unit ==\"arti\":\n",
    "        data=list(zip(years,filenames, titles, objects))\n",
    "        df=pd.DataFrame(data,columns=[\"year\",\"filename\",\"title\",field])\n",
    "        # 合併至年，累計該年數目\n",
    "        data=list(zip(ye,count, count))\n",
    "        df_year=pd.DataFrame(data,columns=[\"year\",\"count\",\"rate\"])   \n",
    "    else:    \n",
    "        data=list(zip(years,filenames, titles, objects))\n",
    "        df=pd.DataFrame(data,columns=[\"year\",\"filename\",\"title\",field])\n",
    "        # 合併至年，累計該年數目與比例\n",
    "        data=list(zip(ye,count, rate))\n",
    "        df_year=pd.DataFrame(data,columns=[\"year\",\"count\",\"rate\"])     \n",
    "\n",
    "    # save，必須要有 data 次目錄     \n",
    "    if path.startswith('/content'):\n",
    "        pth=\"/content/drive/MyDrive/my python/people/\"\n",
    "    else:\n",
    "        pth= \"D:/my python/people/\"\n",
    "\n",
    "    fname=pth+\"data/\"+unit+\"_\"+field+\"_\"+title+\"_year\"+\".xlsx\"\n",
    "    writer = pd.ExcelWriter(fname)\n",
    "    df_year.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "    writer.save()        \n",
    "\n",
    "    try:    # 先存 excel\n",
    "        fname=pth+\"data/\"+unit+\"_\"+field+\"_\"+title+\".xlsx\"\n",
    "        writer = pd.ExcelWriter(fname)\n",
    "        df.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "        writer.save()\n",
    "    except:   # error 再存 pickle\n",
    "        fname=pth+\"data/\"+unit+\"_\"+field+\"_\"+title+\".pkl\"\n",
    "        with open(fname, \"wb\") as fp: \n",
    "            pickle.dump(df, fp)      \n",
    "    return df, df_year     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dba43e-50a3-403a-8a95-6924df61526a",
   "metadata": {
    "id": "25dba43e-50a3-403a-8a95-6924df61526a"
   },
   "source": [
    "## plot_doc_select_year(df_year)\n",
    "* 合併至年度，各年度文章數及比例，折線圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc0f59-35f8-461a-b350-d6e096cb43e1",
   "metadata": {
    "id": "75cc0f59-35f8-461a-b350-d6e096cb43e1"
   },
   "outputs": [],
   "source": [
    "# 各年度的文章數\n",
    "def plot_doc_select_year(df_year,pth=\"/content/drive/MyDrive/my python/people/\"):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    df_year.index=df_year.year\n",
    "    df_year[[\"count\"]].plot(figsize=(16,10))\n",
    "    plt.ylabel('count')\n",
    "    plt.title('count by year')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16,10))\n",
    "    df_year.index=df_year.year\n",
    "    df_year[[\"rate\"]].plot(figsize=(16,10))\n",
    "    plt.ylabel('rate')\n",
    "    plt.title('rate by year')\n",
    "    plt.show()    \n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(16,10))\n",
    "    # 藍色 count\n",
    "    x=df_year.year\n",
    "    c1=df_year[\"count\"]\n",
    "    ax1.plot(x, c1, lw=1, color=\"blue\")   \n",
    "    ax1.set_xticks([1950,1960,1970,1980,1990,2000,2010])\n",
    "    ax1.set_xticklabels([\"1950\",\"1960\",\"1970\",\"1980\",\"1990\",\"2000\",\"2010\"],rotation=0,fontsize=18, color=\"black\")\n",
    "\n",
    "    ax1.set_ylabel(\"count\", fontsize=18, color=\"blue\")\n",
    "    # yticklabels 顏色調整\n",
    "    for label in ax1.get_yticklabels():    \n",
    "        label.set_color(\"blue\")       \n",
    "\n",
    "    c2=df_year.rate       \n",
    "    ax2 = ax1.twinx()            ## twinx\n",
    "    # 紅色 rate\n",
    "    ax2.plot(x, c2, lw=1, color=\"red\")      \n",
    "    ax2.set_ylabel(\"rate %\", fontsize=18, color=\"red\")\n",
    "    # yticklabels 顏色調整\n",
    "    for label in ax2.get_yticklabels():     \n",
    "        label.set_color(\"red\")  \n",
    "    plt.title('count and rate by year', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657a13b-e946-473b-a29d-64a0e99e839a",
   "metadata": {
    "id": "0657a13b-e946-473b-a29d-64a0e99e839a"
   },
   "source": [
    "## select_document_by_key_word(df, field, kw , num, pth)\n",
    "* 從 df 中篩選，含關鍵詞組至少 num 以上者。df 是經過整理過的dataframe 非原始檔。\n",
    "* field 為文件欄位。\n",
    "* kw 關鍵詞組 string or list\n",
    "* num 設定門檻次數\n",
    "* pth 為內訂之資料夾，存檔至 pth/data/ 資料夾 \n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb47bc6-a918-4608-96ef-cac6a7e826a0",
   "metadata": {
    "id": "feb47bc6-a918-4608-96ef-cac6a7e826a0"
   },
   "outputs": [],
   "source": [
    "def select_document_by_key_word(df, field, kw , num=5,  pth=\"/content/drive/MyDrive/my python/people/\" ):\n",
    "    n=0\n",
    "    collect=[]\n",
    "    if type(kw)==str:\n",
    "        for n in range(len(df)):\n",
    "            article=df[field].iloc[n]   \n",
    "            if article.count(kw)>num:\n",
    "                collect.append(n)        \n",
    "    else:    \n",
    "        for n in range(len(df)):\n",
    "            t=0\n",
    "            article=df[field].iloc[n]\n",
    "            for k in kw:    \n",
    "                if article.count(k)>num:\n",
    "                    t+=article.count(k)        \n",
    "            if t>= num:\n",
    "                collect.append(n)\n",
    "    df=df.iloc[collect]\n",
    "    df.index=range(len(df))\n",
    "    title=kw[0]+str(num)\n",
    "\n",
    "    try:\n",
    "        fname=pth+\"data/\"+title+\"_select_by_kw.xlsx\"\n",
    "        writer = pd.ExcelWriter(fname)\n",
    "        df.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "        writer.save()\n",
    "        print (\"save, filename:\", fname)\n",
    "    except:\n",
    "        fname=pth+\"data/\"+title+\"_select_by_kw.pkl\"\n",
    "        with open(fname, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(df, fp)\n",
    "        print (\"save, filename:\", fname)    \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15023011-7b45-4563-9b0b-ebcad7337809",
   "metadata": {
    "id": "15023011-7b45-4563-9b0b-ebcad7337809"
   },
   "source": [
    "## print_doc_select_random(df,num,field)\n",
    "* 隨機列印 doc_select 篩選出來的 dataframe\n",
    "* num 列印的筆數\n",
    "* filed 選擇要列印的欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e4346-cb7d-48b7-8b2f-2ba45affeb8f",
   "metadata": {
    "id": "3f0e4346-cb7d-48b7-8b2f-2ba45affeb8f"
   },
   "outputs": [],
   "source": [
    "def print_doc_select_random(df,num,field):\n",
    "    df_sample=df.sample(num)\n",
    "    for n in range(len(df_sample)):\n",
    "        print(list(df_sample.filename)[n])\n",
    "        print(list(df_sample[field])[n])\n",
    "        print (\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31160429-f9d5-4818-8368-bd80fc80a679",
   "metadata": {
    "id": "31160429-f9d5-4818-8368-bd80fc80a679"
   },
   "source": [
    "## print_filename(filename, field,path)\n",
    "* 列印 filename 所代表的文本，原始資料中 filename 不重覆，代表各篇文章，從 jieba 資料夾中去尋找資訊。\n",
    "* field 可為一個欄位(string) 亦可為多個 (list 格式) \n",
    "* path=\"/content/drive/MyDrive/colab/people/\" 或 D:/my python/people/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95ec24-74ae-4dc5-bc1c-3a027c35c835",
   "metadata": {
    "id": "db95ec24-74ae-4dc5-bc1c-3a027c35c835"
   },
   "outputs": [],
   "source": [
    "def print_filename(filename, field, path):\n",
    "    year=filename[5:9]\n",
    "    fname=str(year)+\"_jieba.xlsx\"\n",
    "    temp=pd.read_excel(path+\"jieba/\"+fname, 0) \n",
    "    print(filename,\"-----------------\")\n",
    "    if type(field)==str:\n",
    "        content=temp[temp.filename==filename][field]\n",
    "        print(list(content))      \n",
    "    else:    \n",
    "        for n in range(len(field)):\n",
    "            content=temp[temp.filename==filename[n]][field[n]]\n",
    "            print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26264ff-2c43-4000-8205-9caa07f1ee97",
   "metadata": {
    "id": "c26264ff-2c43-4000-8205-9caa07f1ee97"
   },
   "source": [
    "# 詞頻與詞對"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88791197-b384-40a2-a641-afa25916b81b",
   "metadata": {
    "id": "88791197-b384-40a2-a641-afa25916b81b",
    "tags": []
   },
   "source": [
    "## corpus_map_year(df,kw, corpus,window, pth,title)\n",
    "* 針對標的詞，統整與其有關的語料庫，增加欄位，顯示字詞出現的頻率。\n",
    "* df 為整理好的 dataframe\n",
    "* kw 為標的詞 list，尋找與標的詞共同出現的詞對\n",
    "* df 必須有 token 欄位，斷好詞空白隔開之string\n",
    "* window 為詞對的距離, 1 為緊鄰\n",
    "* corpus 為語料庫名稱 list\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/\n",
    "* 存檔至 pth+\"data/\"+title+\"_corpus_map_year.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bfc99fa-1b4c-489a-bacd-40ce0694bc18",
   "metadata": {
    "id": "4bfc99fa-1b4c-489a-bacd-40ce0694bc18"
   },
   "outputs": [],
   "source": [
    "# 計算卡方值\n",
    "def chisquare(o11, o12, o21, o22):\n",
    "    n = o11 + o12 + o21 + o22\n",
    "    x_2 = (n * ((o11 * o22 - o12 * o21)**2)) / ((o11 + o12) * (o11 + o21) * (o12 + o22) * (o21 + o22)) \n",
    "    return x_2\n",
    "\n",
    "def corpus_map_year(df,kw, corpus,window, path,title='corpus'):\n",
    "\n",
    "    if 'token' not in list(df.columns):\n",
    "        print ('df 必須要有 token 欄位')\n",
    "        return\n",
    " \n",
    "    # 讀取語料庫, list\n",
    "    corpus_list=corpus.copy()\n",
    "    corpus=[]\n",
    "    for c in corpus_list:\n",
    "        string=\"open(path\" +\"+\"+ \"'corpus/\"+c+\".txt'\"+\",'r',encoding='utf-8').read().split()\"\n",
    "        corpus.append(eval(string))\n",
    "\n",
    "    ## 一年合併成一個文字檔，以年作為分析單位\n",
    "    import re\n",
    "    tokens=[]\n",
    "    years=[]\n",
    "    target=[]\n",
    "    year=sorted(list(df.year.unique()))\n",
    "    for n in range(len(year)):\n",
    "        str_year=''\n",
    "        i=0\n",
    "        df1=df[(df.year==year[n])]     \n",
    "        # 文章 list\n",
    "        token=list(df1.token)  \n",
    "        for t in range(len(token)):\n",
    "            if type(token[t])== str:   \n",
    "                str_year+=\"。\"+token[t]                                                                             \n",
    "                i+=1                             \n",
    "        tokens.append(str_year)      \n",
    "        years.append(year[n])\n",
    "        target.append(i)\n",
    "        print (years[n],\"done\",i,\"found\", end=\" \")    \n",
    "\n",
    "    data=list(zip(years, target,tokens))\n",
    "    df_year=pd.DataFrame(data, columns=[\"year\",\"target\",\"tokens\"])          \n",
    "\n",
    "    ## 詞頻計算，分年度計算\n",
    "    word_count_years=[]\n",
    "    for n in range(len(year)):\n",
    "        # 轉為 list\n",
    "        if pd.isna(tokens[n])==False:\n",
    "            tok=re.split(r'[。 ]',tokens[n])\n",
    "            word_counts = Counter(tok)\n",
    "            word_count_years.append(dict(word_counts.most_common()))    \n",
    "        else:\n",
    "            word_count_years.append(dict())\n",
    "\n",
    "    ## 詞對計算，分年度計算\n",
    "    # 累計配對的次數\n",
    "    pair_count_years=[]\n",
    "    bigrams_years=[]\n",
    "    for n in range(len(year)):    \n",
    "        word_pair_count=Counter()\n",
    "        word1=[]\n",
    "        word2=[]\n",
    "        num_bigrams=0\n",
    "        if pd.isna(tokens[n])==False:\n",
    "            tok=re.split(r'[。 ]',tokens[n])\n",
    "            # i 是啟始位置\n",
    "            for i in range(len(tok)-1):\n",
    "                # j 是間隔 1 表下一個\n",
    "                for j in range(1,window+1):  \n",
    "                    # 啟始加間隔，總共要小於總長度，因為從0起算\n",
    "                    if i+j < len(tok):  \n",
    "                        # 詞對\n",
    "                        (w1,w2)=tok[i],tok[i+j]\n",
    "                        # 大於一個字方納入\n",
    "                        if len(w1)>1 and len(w2)>1:\n",
    "                            word_pair_count[(w1,w2)]+=1                  \n",
    "                            num_bigrams += 1          \n",
    "        bigrams_years.append(num_bigrams)                    \n",
    "        pair_count_years.append(word_pair_count.most_common())   \n",
    "\n",
    "    # 計算卡方值\n",
    "    pair_chi_squares_year=[]\n",
    "    for n in range(len(year)):\n",
    "        pair_chi_squares = Counter()\n",
    "        # 從詞對計算卡方值\n",
    "        for (w1, w2), w1_w2_count in pair_count_years[n]:\n",
    "            if w1 in kw or w2 in kw:\n",
    "                # O12\n",
    "                w1_only_count = word_count_years[n][w1] - w1_w2_count\n",
    "                # O21\n",
    "                w2_only_count = word_count_years[n][w2] - w1_w2_count\n",
    "                # O22\n",
    "                rest_count = bigrams_years[n] - w1_only_count - w2_only_count - w1_w2_count\n",
    "                # O11= w1_w2_count\n",
    "                pair_chi_squares[(w1, w2)] = chisquare(w1_w2_count, w1_only_count, w2_only_count, rest_count)\n",
    "                pair_chi_squares[(w1, w2)] =pair_chi_squares[(w1, w2)]\n",
    "        pair_chi_squares_year.append(pair_chi_squares.most_common())        \n",
    "\n",
    "    # 語料庫圖譜\n",
    "    target=df_year.target\n",
    "    count_total_year=[]\n",
    "    for n in range(len(year)):  \n",
    "        count_total=[]\n",
    "        if target[n]>0:        \n",
    "            # target 是前面已計算的標的詞數目\n",
    "            count_total.append(target[n])\n",
    "            # 卡方值最高的 200個詞對 #######   \n",
    "            temp=pair_chi_squares_year[n]\n",
    "            if len(temp)>=200:\n",
    "                k=100\n",
    "            else:\n",
    "                k=len(temp)\n",
    "            word=[]        \n",
    "            for m in range(k):\n",
    "                if temp[m][0][0] in kw:\n",
    "                    word.append((temp[m][0][1],round(temp[m][1],0)))\n",
    "                else:\n",
    "                    word.append((temp[m][0][0],round(temp[m][1],0)))                                       \n",
    "            count_total.append(word)\n",
    "            # 各年度 pair_count_years 轉為 dictionary\n",
    "            dic=dict(pair_count_years[n])  \n",
    "            # 每個 corpus 依序計算  \n",
    "            for c in range(len(corpus)):     \n",
    "                # w1 w2 其一為標的字詞\n",
    "                for k in kw:\n",
    "                    word1=[]\n",
    "                    word2=[]\n",
    "                    num=[]\n",
    "                    for (w1, w2) in dic:\n",
    "                        # 合於 corpus 條件者選取\n",
    "                        if (w1 == k and w1!=w2 and w2 in corpus[c]) or (w2 == kw and w1!=w2 and w1 in corpus[c]):     \n",
    "                            word1.append(w1)\n",
    "                            word2.append(w2)\n",
    "                            num.append(dic[(w1, w2)])        \n",
    "                    edges=list(zip(word1, word2, num))\n",
    "                    # dataframe\n",
    "                    dfs=pd.DataFrame(edges,columns=[\"word1\",\"word2\",\"num\"])\n",
    "                    # 排序\n",
    "                    dfs=dfs.sort_values(by=['num'],ascending=False)\n",
    "                    # 取前 200 個 #######################\n",
    "                    dfs=dfs.iloc[:200]   \n",
    "                # 詞對數目加入 corpus_count    \n",
    "                temp=dfs.values.tolist()\n",
    "                word=[]\n",
    "                # 只顯示共現詞\n",
    "                for m in range(len(temp)):\n",
    "                    if temp[m][0] in kw:\n",
    "                        word.append((temp[m][1],temp[m][2]))\n",
    "                    else:\n",
    "                        word.append((temp[m][0],temp[m][2]))                                       \n",
    "                count_total.append(word)\n",
    "        else:\n",
    "            count_total.append(target[n])\n",
    "            for k in range(len(corpus)):\n",
    "                count_total.append(np.nan)\n",
    "\n",
    "        # 各個年度計算   \n",
    "        count_total_year.append(count_total)\n",
    "    df_corpus=pd.DataFrame(count_total_year,columns=[title,\"詞對\"]+corpus_list,index=year)\n",
    "    # save excel files\n",
    "    fname=pth+\"data/\"+title+\"_corpus_map_year.xlsx\"\n",
    "    writer = pd.ExcelWriter(fname)\n",
    "    df_corpus.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "    writer.save()\n",
    "    return df_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f4f41-f6a2-477b-be98-4c0eea4bfa41",
   "metadata": {
    "id": "1d5f4f41-f6a2-477b-be98-4c0eea4bfa41"
   },
   "source": [
    "## read_corpus_map_year(kw,pth)\n",
    "* 直接讀取 corpus_map_year 產製並貯存的 excel檔案。\n",
    "* kw 當初的 標的詞\n",
    "* 檔案位置與路徑在 pth+\"data/\"+kw+\"_corpus_map_year.xlsx\"\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c67351-cac9-4a49-ab97-43ff9c062cd1",
   "metadata": {
    "id": "f8c67351-cac9-4a49-ab97-43ff9c062cd1"
   },
   "outputs": [],
   "source": [
    "def read_corpus_map_year( kw, pth=\"/content/drive/MyDrive/colab/people/\" ):\n",
    "    fname=pth+\"data/\"+kw+\"_corpus_map_year.xlsx\"\n",
    "    df_corpus=pd.read_excel(fname, 0)\n",
    "    return df_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584870f-0915-427c-99ef-661defd0bef4",
   "metadata": {
    "id": "6584870f-0915-427c-99ef-661defd0bef4"
   },
   "source": [
    "# 情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35364cea-ee27-4c94-9cf4-fa4a5cae676c",
   "metadata": {
    "id": "35364cea-ee27-4c94-9cf4-fa4a5cae676c"
   },
   "source": [
    "## sentiment_analysis(df, field, path, title, rolling)\n",
    "* 針對整理好的 df, 計算情感詞，要有褒獎詞、貶抑詞、反革命語料庫，進行情感分析，繪製折線圖。\n",
    "* field 指定欄位，可為 ['textrank','tfidf','token'] 三者其中之一。\n",
    "* 語料庫所在的位置為 pth +\"corpus/\"\n",
    "* title 為標題\n",
    "* rolling 為前面 n 個數值平均，目的在平滑化\n",
    "* 讀取情緒詞字典\n",
    "* path=\"/content/drive/MyDrive/colab/people/\"\n",
    "* 存檔至 pth+\"data/\"+title+\"_pn.xlsx\"\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332dd7e8-54e9-4942-b9af-145f023e9054",
   "metadata": {
    "id": "332dd7e8-54e9-4942-b9af-145f023e9054"
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis( df,field,path=\"/content/drive/MyDrive/colab/people/\", title='sentiment_analysis', rolling=1):\n",
    "    if field not in ['textrank','tfidf','token']:\n",
    "        print ('field 必須是 textrank,tfidf,token 三者之一')\n",
    "        return\n",
    "    import pandas as pd\n",
    "    from collections import Counter\n",
    "    # 情緒詞\n",
    "    ph=path +\"corpus/\"\n",
    "    file_name = ph + \"反革命.txt\"\n",
    "    反革命= open(file_name,'r',encoding='utf-8').read()\n",
    "    反革命=list(set(反革命.split()))     \n",
    "    file_name = ph + \"貶抑詞.txt\"\n",
    "    貶抑詞= open(file_name,'r',encoding='utf-8').read()\n",
    "    貶抑詞 =list(set(貶抑詞.split())) \n",
    "    file_name = ph + \"褒獎詞.txt\"\n",
    "    褒獎詞= open(file_name,'r',encoding='utf-8').read()\n",
    "    褒獎詞 =list(set(褒獎詞.split())) \n",
    "\n",
    "    # 計算每一筆資料的情感詞數目\n",
    "    pos=[];neg=[];meeting=[];people=[];position=[];anti=[]\n",
    "    n_pos=[];n_neg=[];n_anti=[]\n",
    "    for n in range(len(df)):\n",
    "        sent=df[field].iloc[n]\n",
    "        if field in ['textrank','tfidf']:\n",
    "            token=sent.split(',')\n",
    "        else:\n",
    "            token=sent.split('')\n",
    "        # words\n",
    "        pos.append(list(set(token).intersection(set(褒獎詞))))\n",
    "        neg.append(list(set(token).intersection(set(貶抑詞))))\n",
    "        anti.append(list(set(token).intersection(set(反革命))))\n",
    "        # count\n",
    "        n_pos.append(len((list(set(token).intersection(set(褒獎詞))))))\n",
    "        n_neg.append(len((list(set(token).intersection(set(貶抑詞)))))) \n",
    "        n_anti.append(len((list(set(token).intersection(set(反革命)))))) \n",
    "    # 字詞\n",
    "    df[\"褒獎詞\"]=pos\n",
    "    df[\"貶抑詞\"]=neg\n",
    "    df[\"反革命\"]=anti\n",
    "    # 數量\n",
    "    df[\"pos\"]=n_pos\n",
    "    df[\"neg\"]=n_neg\n",
    "    df[\"anti\"]=n_anti\n",
    "\n",
    "    # 彙整到年\n",
    "    pos_year=[];neg_year=[];num_year=[];anti_year=[]\n",
    "    褒獎詞=[]\n",
    "    貶抑詞=[]\n",
    "    反革命=[]\n",
    "    year=sorted(list(df.year.unique()))\n",
    "    for y in year:\n",
    "        # 各年度文件總數\n",
    "        num=df[df.year==y].pos.count()\n",
    "        num_year.append(num)\n",
    "        # 各年度正向詞總數\n",
    "        pos_total=df[df.year==y].pos.sum()\n",
    "        pos_year.append(pos_total)\n",
    "        # 各年度負向詞總數\n",
    "        neg_total=df[df.year==y].neg.sum()\n",
    "        neg_year.append(neg_total)   \n",
    "        # 各年度反革命詞總數\n",
    "        anti_total=df[df.year==y].anti.sum()\n",
    "        anti_year.append(anti_total)  \n",
    "        \n",
    "        # 關鍵詞\n",
    "        df_y=df[df.year==y]\n",
    "        褒=[];貶=[];反=[]\n",
    "        for n in range(len(df_y)):\n",
    "            if len(df_y.褒獎詞.iloc[n])!=0:\n",
    "                褒+=df_y.褒獎詞.iloc[n]\n",
    "            if len(df_y.貶抑詞.iloc[n])!=0:\n",
    "                貶+=df_y.貶抑詞.iloc[n]\n",
    "            if len(df_y.反革命.iloc[n])!=0:    \n",
    "                反+=df_y.反革命.iloc[n]          \n",
    "        褒=list(Counter(褒).items()) \n",
    "        貶=list(Counter(貶).items())\n",
    "        反=list(Counter(反).items())\n",
    "        褒獎詞.append(褒)\n",
    "        貶抑詞.append(貶)\n",
    "        反革命.append(反)    \n",
    "    df_pn=pd.DataFrame(list(zip(year,褒獎詞,貶抑詞,反革命,num_year,pos_year, neg_year,anti_year)),columns=[\"year\",\"褒獎詞\",\"貶抑詞\",\"反革命\",\"count\",\"pos\",\"neg\",\"anti\"])\n",
    "    \n",
    "    df_pn[\"pos%\"]=df_pn[\"pos\"]/df_pn[\"count\"]\n",
    "    df_pn[\"neg%\"]=df_pn[\"neg\"]/df_pn[\"count\"]\n",
    "\n",
    "    # line chart ------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    ax.plot(year, df_pn[\"pos%\"].rolling(rolling).mean(),label=\"pos%\")\n",
    "    ax.plot(year, df_pn[\"neg%\"].rolling(rolling).mean(),label=\"neg%\")\n",
    "    ax.set_xticks([1950,1960,1970,1980,1990,2000,2010])\n",
    "    ax.legend();  \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('value')    # y lobel\n",
    "    # 標題\n",
    "    ax.set_title(title);   # title\n",
    "    plt.show()    \n",
    "    if path.startswith('/content'):\n",
    "        pth=\"/content/drive/MyDrive/my python/people/\"\n",
    "    else:\n",
    "        pth= \"D:/my python/people/\"\n",
    "    # save\n",
    "    fname=pth+\"data/\"+title+\"_pn.xlsx\"\n",
    "    writer = pd.ExcelWriter(fname)\n",
    "    df_pn.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "    writer.save()\n",
    "    return df_pn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044e853-aade-492c-b22e-00ca708937ca",
   "metadata": {
    "id": "c044e853-aade-492c-b22e-00ca708937ca"
   },
   "source": [
    "## read_sentiment_analysis(pth,title)\n",
    "* 直接讀取 sentiment_analysis 產製並貯存的 excel檔案。\n",
    "* title 當初的 title\n",
    "* pth=\"/content/drive/MyDrive/colab/people/\" 或 D:/my python/people/\n",
    "* 檔案位置與路徑在 pth+\"data/\"+title+\"_pn.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49496b4-e4c2-4d80-aafb-3d63e5450002",
   "metadata": {
    "id": "d49496b4-e4c2-4d80-aafb-3d63e5450002"
   },
   "outputs": [],
   "source": [
    "def read_sentiment_analysis(df, pth=\"/content/drive/MyDrive/my python/people/\", title='sentiment_analysis'):\n",
    "    import pandas as pd\n",
    "    fname=path+\"data/\"+title+\"_pn.xlsx\"\n",
    "    df_pn=pd.read_excel(fname, 0)\n",
    "    return df_pn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd96a6-16ab-4f47-8e6f-42bc3d164240",
   "metadata": {
    "id": "a3dd96a6-16ab-4f47-8e6f-42bc3d164240"
   },
   "source": [
    "# 主題分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdbcc32-1a16-4b4b-a4ec-aa8428c434d5",
   "metadata": {
    "id": "4bdbcc32-1a16-4b4b-a4ec-aa8428c434d5",
    "tags": []
   },
   "source": [
    "## topic_model(df, field,num_i, method,pth, title)\n",
    "* 針對 df 內的文本進行主題分析，必須有已斷好詞的欄位。\n",
    "* method 有 svd,lda, nmf(Non-negative Matrix Factorization) 三者之一\n",
    "* field 為斷詞後之欄位名稱，'textrank','tfidf','token' 三者之一。\n",
    "* num_i 主題數目,至多十個類別\n",
    "* 檔案存於 pth+\"data/\"+title+\"_df.xlsx\"，如果檔案太大，則目動轉為 pth+\"data/\"+title+\"_df.pkl\"\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/\n",
    "* title 為標題\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d111c-2057-4498-ba07-60879f059561",
   "metadata": {
    "id": "468d111c-2057-4498-ba07-60879f059561",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def topic_model(df, field,num_i, method, pth, title):\n",
    "    if field not in ['textrank','tfidf','token']:\n",
    "        print ('field 必是須 textrank,tfidf,token 三者之一')\n",
    "        return\n",
    "    # 文章字詞矩陣\n",
    "    print ('tfidf......')\n",
    "    # list of 空白隔開之 string\n",
    "    if field in ['textrank','tfidf']:\n",
    "        df[field]=df[field].str.replace(',',' ')   \n",
    "    doc_list=df[field]\n",
    "    \n",
    "    ngram=(1,1)\n",
    "    vectorizer = TfidfVectorizer(min_df = 1,ngram_range= ngram)\n",
    "    dtm_tfidf = vectorizer.fit_transform(doc_list) # sparse matrix\n",
    "    # 字詞的編號\n",
    "    terms=vectorizer.vocabulary_\n",
    "    # 字詞的 list\n",
    "    term_list=vectorizer.get_feature_names_out()\n",
    "    \n",
    "    print ('extract topics ......')\n",
    "    if method==\"svd\":\n",
    "        lsa = TruncatedSVD(num_i, algorithm = 'arpack')   # object\n",
    "    elif method==\"lda\":\n",
    "        lsa=LatentDirichletAllocation(num_i)\n",
    "    elif method==\"nmf\":    \n",
    "        lsa= NMF(num_i)   # 各投影值只有正值，沒有負值    \n",
    "\n",
    "    dtm_lsa = lsa.fit_transform(dtm_tfidf)  # 加權\n",
    "    dtm_com = lsa.fit(dtm_tfidf)  # object, not numpy array\n",
    "\n",
    "    # components\n",
    "    comp=dtm_com.components_\n",
    "    print(\"維度數\",len(comp))\n",
    "    print(\"字詞數\",len(comp[0]))\n",
    "\n",
    "    # 每個維度軸都是各字詞的線性組合，每個維度的構成\n",
    "    df_concept=pd.DataFrame(comp,columns= term_list)\n",
    "    df_concept=df_concept.transpose()\n",
    "\n",
    "    # 各維度字詞，依重要性大小排序\n",
    "    con=[]\n",
    "    sig=[]\n",
    "    for n in range(len(df_concept.columns)):\n",
    "        # 由大至小排序\n",
    "        df_order= df_concept.sort_values(by=n, ascending=False)\n",
    "        con.append(list(df_order.index)[0:500])\n",
    "        sig.append(list(df_order[n])[0:500])\n",
    "        \n",
    "    # 印出各軸的重要字詞\n",
    "    for n in range(len(con)):\n",
    "        print (n,con[n][:50])          \n",
    "\n",
    "    # 轉成 dataframe 格式\n",
    "    df_topic=pd.DataFrame(con).T\n",
    "    df_topic\n",
    "    \n",
    "    print ('df_lsa......')\n",
    "    # 依投影量最大的主題歸類\n",
    "    cate=[]\n",
    "    for n in range(len(dtm_lsa)):\n",
    "        ma=max(dtm_lsa[n])\n",
    "        po=list(dtm_lsa[n]).index(ma)\n",
    "        cate.append(po)\n",
    "    df['cate']=cate\n",
    "   \n",
    "    columns=[]\n",
    "    for n in range(num_i):\n",
    "        columns.append(\"topic\"+str(n))\n",
    "        \n",
    "    df_lsa=pd.DataFrame(dtm_lsa,columns=columns)\n",
    "    df_lsa[\"year\"]=df.year\n",
    "    \n",
    "    # save，必須要有 data 次目錄    \n",
    "    print (\"save......\")    \n",
    "    fname=pth+\"data/\"+title+\"_df_lsa.xlsx\"\n",
    "    writer = pd.ExcelWriter(fname)\n",
    "    df_lsa.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "    writer.save() \n",
    "    print ('save', fname)\n",
    "       \n",
    "    fname=pth+\"data/\"+title+\"_df_topic.xlsx\"\n",
    "    writer = pd.ExcelWriter(fname)\n",
    "    df_topic.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "    writer.save() \n",
    "    print ('save', fname)\n",
    "    try:\n",
    "        fname=pth+\"data/\"+title+\"_df.xlsx\"\n",
    "        writer = pd.ExcelWriter(fname)\n",
    "        df.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "        writer.save()\n",
    "        print ('save', fname)\n",
    "    except:\n",
    "        fname=pth+\"data/\"+title+\"_df.pkl\"\n",
    "        with open(fname, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(df, fp)   \n",
    "        print ('save', fname)    \n",
    "    return df, df_topic,df_lsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1de268-a473-4c9f-8dd8-decb936b99eb",
   "metadata": {
    "id": "0b1de268-a473-4c9f-8dd8-decb936b99eb"
   },
   "source": [
    "## read_topic_model(pth,title)\n",
    "* 直接讀取 topic 產製並貯存的 excel檔案。\n",
    "* title 當初的 title\n",
    "* 檔案位置與路徑在 pth+\"data/\"+title+\"_pn.xlsx\"\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb7a50-01a7-49b3-9a32-a567899cfda5",
   "metadata": {
    "id": "15bb7a50-01a7-49b3-9a32-a567899cfda5"
   },
   "outputs": [],
   "source": [
    "def read_topic_model(pth,title):\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        fname=pth+\"data/\"+title+\"_\"+\"df\"+\".xlsx\"\n",
    "        df=pd.read_excel(fname, 0)\n",
    "    except:\n",
    "        fname=pth+\"data/\"+title+\"_\"+\"df\"+\".pkl\"\n",
    "        with open(fname, \"rb\") as fp:   \n",
    "            df = pickle.load(fp)              \n",
    "    fname=pth+\"data/\"+title+\"_\"+\"df_topic\"+\".xlsx\"\n",
    "    df_topic=pd.read_excel(fname, 0)    \n",
    "    \n",
    "    fname=pth+\"data/\"+title+\"_\"+\"df_lsa\"+\".xlsx\"\n",
    "    df_lsa=pd.read_excel(fname, 0)\n",
    "    return df, df_topic,df_lsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a255469-438d-464d-b004-2504628888e9",
   "metadata": {
    "id": "7a255469-438d-464d-b004-2504628888e9"
   },
   "source": [
    "## plot_topic_model(df_lsa,labels, rolling)\n",
    "* 針對 topic_model 的結果，繪製時間趨勢變化圖。 \n",
    "* df_lsa 為 topic_model 分析之結果\n",
    "* labels 是研究者自行給主題的命名 list\n",
    "* rolling 為前者 n 個數值的平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6cddc-7739-47ad-95e4-11f356b577cb",
   "metadata": {
    "id": "a3c6cddc-7739-47ad-95e4-11f356b577cb"
   },
   "outputs": [],
   "source": [
    "# 主題佔比赹勢圖，自行調整類別數\n",
    "def plot_topic_model(df_lsa,labels,rolling):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    # 直接繪圖，毋須 plot.show()\n",
    "    %matplotlib inline  \n",
    "    # 中文設定1: 楷體字\n",
    "    from matplotlib import rcParams\n",
    "    rcParams[\"font.sans-serif\"] = [\"kaiu\"] \n",
    "    rcParams[\"font.family\"] =\"DFKai-sb\"\n",
    "    \n",
    "    year=sorted(list(df_lsa.year.unique()))\n",
    "    num_i=0\n",
    "    for t in list(df_lsa.columns):\n",
    "        if t.startswith(\"topic\")==True:\n",
    "            num_i+=1\n",
    "    if labels==[]:\n",
    "        labels=list(range(num_i))\n",
    "        \n",
    "    # 欄位名稱\n",
    "    columns=[]\n",
    "    for n in range(num_i):\n",
    "        columns.append(\"topic\"+str(n)) \n",
    "        \n",
    "    # 各年各軸投影量加總    \n",
    "    totals=[]\n",
    "    for y in range(len(year)):\n",
    "        total=0\n",
    "        for n in range(num_i):\n",
    "            df_tem=df_lsa[df_lsa[\"year\"]==year[y]]\n",
    "            total+=df_tem[columns[n]].sum()            \n",
    "        totals.append(total)\n",
    "        \n",
    "    # 計算各軸投影百分比    \n",
    "    ts=[]    \n",
    "    for n in range(num_i):    \n",
    "        t=[]\n",
    "        for y in range(len(year)):    \n",
    "            df_tem=df_lsa[df_lsa[\"year\"]==year[y]]\n",
    "            t.append(df_tem[columns[n]].sum()/totals[y])\n",
    "        ts.append(list(pd.Series(t).rolling(rolling).mean()))    \n",
    "\n",
    "    # 折線圖 --------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    for c in range(len(ts)): \n",
    "        ax.plot(year, ts[c], label=labels[c])\n",
    "    ax.set_xticks([1950,1960,1970,1980,1990,2000,2010])\n",
    "    # 圖例位置由程式自己決定\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5)) \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('value')    # y lobel\n",
    "    # 標題\n",
    "    ax.set_title(\"折線圖\");   # title\n",
    "    plt.show()\n",
    "    \n",
    "    # 疊加圖 --------------------------------------------\n",
    "    # 圖區大小\n",
    "    plt.figure(figsize=(16,10))  \n",
    "    # 繪疊加圖\n",
    "    if num_i==2:\n",
    "        plt.stackplot(year, ts[0],ts[1], labels=labels)\n",
    "    elif num_i==3:\n",
    "        plt.stackplot(year, ts[0],ts[1],ts[2], labels=labels)\n",
    "    elif num_i==4:\n",
    "        plt.stackplot(year, ts[0],ts[1],ts[2],ts[3], labels=labels)        \n",
    "    elif num_i==5:\n",
    "        plt.stackplot(year, ts[0],ts[1],ts[2],ts[3],ts[4], labels=labels)         \n",
    "    elif num_i==6:\n",
    "        plt.stackplot(year, ts[0],ts[1],ts[2],ts[3],ts[4],ts[5], labels=labels)           \n",
    "    elif num_i==7:\n",
    "        plt.stackplot(year, ts[0],ts[1],ts[2],ts[3],ts[4],ts[5],ts[6], labels=labels)         \n",
    "    elif num_i==8:\n",
    "        plt.stackplot(year, ts[0],ts[1],ts[2],ts[3],ts[4],ts[5],ts[6],ts[7],labels=labels)          \n",
    "    elif num_i==9:\n",
    "        plt.stackplot(year, ts[0],ts[1],ts[2],ts[3],ts[4],ts[5],ts[6],ts[7],ts[8],labels=labels)  \n",
    "    elif num_i==10:\n",
    "        plt.stackplot(year, ts[0],ts[1],ts[2],ts[3],ts[4],ts[5],ts[6],ts[7],ts[8],ts[9],labels=labels) \n",
    "    # X 軸尺度\n",
    "    # plt.xticks(year)\n",
    "    # X 軸標題\n",
    "    plt.xlabel('year')   \n",
    "    # Y 軸標題律\n",
    "    plt.ylabel('value')   \n",
    "    # 圖例\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5)) \n",
    "    # 標題\n",
    "    plt.title('疊加圖')    \n",
    "    plt.show()\n",
    "    \n",
    "    # 圓餅圖 --------------------------------------------\n",
    "    df_trend=pd.DataFrame(ts)\n",
    "    # num_i=len(labels)\n",
    "    size=[]\n",
    "    for n in range(num_i):\n",
    "        total=df_trend.iloc[n].sum()\n",
    "        size.append(total)\n",
    "    # 圖面大小\n",
    "    plt.figure(figsize=(10,10))\n",
    "    # 圖餅圖 pie，變數值，標籤，數字格式，加陰影，旋轉角度\n",
    "    plt.pie(size, labels=labels, autopct='%1.1f%%', shadow=True, startangle=100) \n",
    "    # 正圓形\n",
    "    plt.axis('equal')  \n",
    "    plt.show()\n",
    "    \n",
    "    # 長條圖 --------------------------------------------\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(labels,size) \n",
    "    plt.show()\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c2756-7b88-4083-a1e6-5481878a4238",
   "metadata": {
    "id": "fb0c2756-7b88-4083-a1e6-5481878a4238"
   },
   "source": [
    "## plot_topic_model_pie(labels, ts)\n",
    "* 針對 plot_topic_model 的結果，繪製圓餅圖。 \n",
    "* ts 為 plot_topic_model 分析之結果\n",
    "* labels 是研究者自行給主題的命名 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f563095-b556-4233-8d76-c3fad199fa85",
   "metadata": {
    "id": "6f563095-b556-4233-8d76-c3fad199fa85"
   },
   "outputs": [],
   "source": [
    "# 圓餅圖\n",
    "def plot_topic_model_pie(labels, ts):   \n",
    "    df_trend=pd.DataFrame(ts)\n",
    "    num_i=len(labels)\n",
    "    size=[]\n",
    "    for n in range(num_i):\n",
    "        total=df_trend.iloc[n].sum()\n",
    "        size.append(total)\n",
    "    # 圖面大小\n",
    "    plt.figure(figsize=(10,10))\n",
    "    # 圖餅圖 pie，變數值，標籤，數字格式，加陰影，旋轉角度\n",
    "    plt.pie(size, labels=labels, autopct='%1.1f%%', shadow=True, startangle=100) \n",
    "    # 正圓形\n",
    "    plt.axis('equal')  \n",
    "    plt.show()\n",
    "    return df_trend, size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde6557b-3eb4-4d16-a4f8-750966c5faaf",
   "metadata": {
    "id": "fde6557b-3eb4-4d16-a4f8-750966c5faaf"
   },
   "source": [
    "## plot_topic_model_bar(labels,size)\n",
    "* plot_topic_model_pei 產生之 size，繪製長條圖\n",
    "* size 為 plot_topic_model_pei 產生之 size 變數\n",
    "* lables 為自行命名之類別名稱，list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2dbe30-ccfa-4f98-b3ff-b9902066746d",
   "metadata": {
    "id": "ed2dbe30-ccfa-4f98-b3ff-b9902066746d"
   },
   "outputs": [],
   "source": [
    "# 長條圖\n",
    "def plot_topic_model_bar(labels,size):  \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(labels,size) \n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5pGOHCGpEtxZ",
   "metadata": {
    "id": "5pGOHCGpEtxZ"
   },
   "source": [
    "# 文章分群"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2979b7-b027-4d2f-9303-5d5e5f46ee5a",
   "metadata": {
    "id": "5d2979b7-b027-4d2f-9303-5d5e5f46ee5a",
    "tags": []
   },
   "source": [
    "## cluster_analysis(df, field,pth,title, num_i=100, method='svd', n_clus=10)\n",
    "* 文章分群，df 為 dataframe，要有 year, token 欄位\n",
    "* num_i為降維之維度數，n_clus 為分群之數目\n",
    "* 資料多，耗時久。\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45509195-c842-4204-bb5a-2ca62f323e84",
   "metadata": {
    "id": "45509195-c842-4204-bb5a-2ca62f323e84"
   },
   "outputs": [],
   "source": [
    "def plot_cluster_analysis(df, labels ,pth=\"/content/drive/MyDrive/my python/people/\",rolling=1):\n",
    "    # df 以文本為單位，必須有 year, cluster 欄位，自動計算 rate, count by year\n",
    "    # cluster 為類別，從0 起算之數字序號\n",
    "    # line, bar, pie, agrigate 全部都會畫\n",
    "    import matplotlib.pyplot as plt \n",
    "    # 中文設定\n",
    "    from matplotlib import rcParams\n",
    "    rcParams[\"font.sans-serif\"] = [\"kaiu\"] \n",
    "    rcParams[\"font.family\"] =\"DFKai-sb\"\n",
    "    \n",
    "    ## 計算各類文章的比例\n",
    "    print (\"calculate rate......\")\n",
    "    count=[]\n",
    "    year=sorted(list(df.year.unique()))\n",
    "    n_clus=len(df.cluster.unique()) \n",
    "    if labels==[]:\n",
    "        labels=list(range(n_clus))\n",
    "    \n",
    "    for y in year:\n",
    "        df_year=df[df.year==y]\n",
    "        a=df_year['cluster'].value_counts() \n",
    "        df_cluster = pd.DataFrame(a).sort_index(ascending=True)\n",
    "        # 如果某個年度， 群組數目有缺\n",
    "        if n_clus>len(df_cluster):\n",
    "            count_y=[]\n",
    "            for n in range(n_clus):\n",
    "                try:           \n",
    "                    count_y.append(int(df_cluster.loc[n]))   \n",
    "                except:\n",
    "                    count_y.append(0)       \n",
    "        else:                   \n",
    "            count_y=list(df_cluster.cluster)\n",
    "\n",
    "        rate_y=[]\n",
    "        for c in count_y:    \n",
    "            if sum(count_y)>0:\n",
    "                # 百分比\n",
    "                rate=c/sum(count_y)                    \n",
    "            else:\n",
    "                rate=0\n",
    "            rate_y.append(rate)    \n",
    "        count.append(rate_y)\n",
    "        print (y,end=\" \")\n",
    "    df_rate=pd.DataFrame(count, index=year,columns=range(n_clus))\n",
    "\n",
    "    ## 加總各類文章篇數\n",
    "    print (\"count ......\")\n",
    "    count=[]\n",
    "    for y in year:\n",
    "        df_year=df[df.year==y]\n",
    "        a=df_year['cluster'].value_counts() \n",
    "        df_cluster = pd.DataFrame(a).sort_index(ascending=True)\n",
    "        # 如果某個年度，某個群數為0\n",
    "        if len(year)>len(df_cluster):\n",
    "            count_y=[]\n",
    "            for n in range(n_clus):\n",
    "                try:           \n",
    "                    count_y.append(int(df_cluster.loc[n]))   \n",
    "                except:\n",
    "                    count_y.append(0)       \n",
    "        else:                   \n",
    "            count_y=list(df_cluster.cluster)   \n",
    "        count.append(count_y)\n",
    "        print (y,end=\" \")\n",
    "    df_count=pd.DataFrame(count, index=year,columns=range(n_clus))\n",
    "    \n",
    "    # 如果有 labels, cluster 替代為 labels\n",
    "    if len(labels)!=n_clus:\n",
    "         labels=list(range(n_clus)) \n",
    "\n",
    "    ## 分群數量，時間趨勢圖 ----------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    year=sorted(set(df.year.unique()))\n",
    "    for c in range(n_clus): \n",
    "        ax.plot(year, df_count[c].rolling(rolling).mean(), label=labels[c])      \n",
    "    ax.set_xticks([1950,1960,1970,1980,1990,2000,2010])\n",
    "\n",
    "    # 圖例位置由程式自己決定\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5)) \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('freq.')    # y lobel\n",
    "    ax.set_title('數量 by year and cluster');   # title\n",
    "    plt.show()\n",
    "\n",
    "    ## 分群比例，時間趨勢圖 -----------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    year=sorted(set(df.year.unique()))\n",
    "    for c in range(n_clus): \n",
    "         ax.plot(year, df_rate[c].rolling(rolling).mean(), label=labels[c])      \n",
    "    ax.set_xticks([1950,1960,1970,1980,1990,2000,2010])\n",
    "\n",
    "    ax.legend(loc=1);  \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('freq.')    # y lobel\n",
    "    ax.set_title('比例 by year and cluster');   # title\n",
    "    plt.show()\n",
    "\n",
    "    ## 各群文章 count 時間疊加圖 -------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    ax.set_xticks([1950,1960,1970,1980,1990,2000,2010])\n",
    "    for n in range(len(labels)):\n",
    "        df_count[n]=df_count[n].rolling(rolling).mean()\n",
    "    if n_clus==2:\n",
    "        plt.stackplot(year, df_count[0],df_count[1], labels=labels)\n",
    "    elif n_clus==3:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2], labels=labels)\n",
    "    elif n_clus==4:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3], labels=labels)        \n",
    "    elif n_clus==5:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4], labels=labels)         \n",
    "    elif n_clus==6:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4],df_count[5], labels=labels)           \n",
    "    elif n_clus==7:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4],df_count[5],df_count[6], labels=labels)         \n",
    "    elif n_clus==8:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4],df_count[5],df_count[6],df_count[7],labels=labels)          \n",
    "    elif n_clus==9:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4],df_count[5],df_count[6],df_count[7],df_count[8],labels=labels)  \n",
    "    elif n_clus==10:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4],df_count[5],df_count[6],df_count[7],df_count[8],df_count[9],labels=labels)     \n",
    "\n",
    "    # 圖例位置由程式自己決定\n",
    "    ax.legend(loc=0);  \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('number')    # y lobel\n",
    "    ax.set_title('數量 by year and cluster');   # title\n",
    "    plt.show()\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    ax.set_xticks([1950,1960,1970,1980,2000,2010])\n",
    "\n",
    "    # rate繪疊加圖 \n",
    "    for n in range(len(labels)):\n",
    "        df_rate[n]=df_rate[n].rolling(rolling).mean()\n",
    "                \n",
    "    if n_clus==2:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1], labels=labels)\n",
    "    elif n_clus==3:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2], labels=labels)\n",
    "    elif n_clus==4:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3], labels=labels)        \n",
    "    elif n_clus==5:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4], labels=labels)         \n",
    "    elif n_clus==6:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4],df_rate[5], labels=labels)           \n",
    "    elif n_clus==7:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4],df_rate[5],df_rate[6], labels=labels)         \n",
    "    elif n_clus==8:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4],df_rate[5],df_rate[6],df_rate[7],labels=labels)          \n",
    "    elif n_clus==9:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4],df_rate[5],df_rate[6],df_rate[7],df_rate[8],labels=labels)  \n",
    "    elif n_clus==10:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4],df_rate[5],df_rate[6],df_rate[7],df_rate[8],df_rate[9],labels=labels)      \n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5)) \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('number')    # y lobel\n",
    "    ax.set_title('比例 by year and cluster');   # title\n",
    "    plt.show()\n",
    "\n",
    "    # 圓餅圖 --------------------------------------------\n",
    "    # percent of each cluster\n",
    "    cluster=list(df.cluster)\n",
    "    percent=[]\n",
    "    for c in range(n_clus): \n",
    "        percent.append(cluster.count(c)/len(cluster))\n",
    "\n",
    "    patches, texts = plt.pie(percent,labels=labels,shadow=False, startangle=90)\n",
    "    #patches, texts = plt.pie(percent,labels=labels,autopct='%1.1f%%',shadow=False, startangle=90)\n",
    "    plt.legend(patches, labels, loc=\"best\")\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 長條圖 --------------------------------------------\n",
    "    # percent of each cluster\n",
    "    total=[]\n",
    "    for c in range(n_clus): \n",
    "        total.append(cluster.count(c))\n",
    "    plt.bar(labels,total)\n",
    "    plt.show()\n",
    "    return df_count, df_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3324bc-7208-4e6e-91dd-9f0935f168cf",
   "metadata": {
    "id": "9b3324bc-7208-4e6e-91dd-9f0935f168cf"
   },
   "source": [
    "## read_cluster_analysis(pth,title)\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127eea1-c69b-4704-b8a9-e118752b6aec",
   "metadata": {
    "id": "d127eea1-c69b-4704-b8a9-e118752b6aec"
   },
   "outputs": [],
   "source": [
    "def read_cluster_analysis(pth=\"/content/drive/MyDrive/my python/people/\",title='cluster_analysis'):\n",
    "    try:\n",
    "        fname=pth+\"data/\"+title+\"_\"+\"df\"+\".xlsx\"\n",
    "        df=pd.read_excel(fname, 0)\n",
    "    except:\n",
    "        fname=pth+\"data/\"+title+\"_\"+\"df\"+\".pkl\"\n",
    "        with open(fname, \"rb\") as fp:  \n",
    "            df = pickle.load(fp)         \n",
    "\n",
    "    fname=pth+\"data/\"+title+\"_\"+\"clus_token\"+\".pkl\"     \n",
    "    with open(fname, \"rb\") as fp:   \n",
    "        clus_token = pickle.load(fp)        \n",
    "           \n",
    "    fname=pth+\"data/\"+title+\"_\"+\"df_rate\"+\".xlsx\"\n",
    "    df_rate=pd.read_excel(fname, 0)    \n",
    "    \n",
    "    fname=pth+\"data/\"+title+\"_\"+\"df_count\"+\".xlsx\"\n",
    "    df_count=pd.read_excel(fname, 0)\n",
    "\n",
    "    fname=pth+\"data/\"+title+\"_\"+\"df_clus_most\"+\".xlsx\"\n",
    "    df_clus_most=pd.read_excel(fname, 0)\n",
    "    \n",
    "    return df, clus_token, df_rate, df_count,df_clus_most"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9951970-0972-4977-b8c1-0fa388ad7a6b",
   "metadata": {
    "id": "d9951970-0972-4977-b8c1-0fa388ad7a6b"
   },
   "source": [
    "## plot_cluster_analysis(df, labels,pth,rolling)\n",
    "* df 必須有 cluster, year 欄位，至多十個類別\n",
    "* label list 類別命名\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91df24f-8930-47d8-97f4-603b8f9fc3bc",
   "metadata": {
    "id": "d91df24f-8930-47d8-97f4-603b8f9fc3bc"
   },
   "outputs": [],
   "source": [
    "def plot_cluster_analysis(df, labels ,pth=\"/content/drive/MyDrive/my python/people/\",rolling=1):\n",
    "    # df 以文本為單位，必須有 year, cluster 欄位，自動計算 rate, count by year\n",
    "    # cluster 為類別，從0 起算之數字序號\n",
    "    # line, bar, pie, agrigate 全部都會畫\n",
    "    import matplotlib.pyplot as plt \n",
    "    # 中文設定\n",
    "    from matplotlib import rcParams\n",
    "    rcParams[\"font.sans-serif\"] = [\"kaiu\"] \n",
    "    rcParams[\"font.family\"] =\"DFKai-sb\"\n",
    "    \n",
    "    ## 計算各類文章的比例\n",
    "    print (\"calculate rate......\")\n",
    "    count=[]\n",
    "    year=sorted(list(df.year.unique()))\n",
    "    n_clus=len(df.cluster.unique()) \n",
    "    if labels==[]:\n",
    "        labels=list(range(n_clus))\n",
    "    \n",
    "    for y in year:\n",
    "        df_year=df[df.year==y]\n",
    "        a=df_year['cluster'].value_counts() \n",
    "        df_cluster = pd.DataFrame(a).sort_index(ascending=True)\n",
    "        # 如果某個年度， 群組數目有缺\n",
    "        if n_clus>len(df_cluster):\n",
    "            count_y=[]\n",
    "            for n in range(n_clus):\n",
    "                try:           \n",
    "                    count_y.append(int(df_cluster.loc[n]))   \n",
    "                except:\n",
    "                    count_y.append(0)       \n",
    "        else:                   \n",
    "            count_y=list(df_cluster.cluster)\n",
    "\n",
    "        rate_y=[]\n",
    "        for c in count_y:    \n",
    "            if sum(count_y)>0:\n",
    "                # 百分比\n",
    "                rate=c/sum(count_y)                    \n",
    "            else:\n",
    "                rate=0\n",
    "            rate_y.append(rate)    \n",
    "        count.append(rate_y)\n",
    "        print (y,end=\" \")\n",
    "    df_rate=pd.DataFrame(count, index=year,columns=range(n_clus))\n",
    "\n",
    "    ## 加總各類文章篇數\n",
    "    print (\"count ......\")\n",
    "    count=[]\n",
    "    for y in year:\n",
    "        df_year=df[df.year==y]\n",
    "        a=df_year['cluster'].value_counts() \n",
    "        df_cluster = pd.DataFrame(a).sort_index(ascending=True)\n",
    "        # 如果某個年度，某個群數為0\n",
    "        if len(year)>len(df_cluster):\n",
    "            count_y=[]\n",
    "            for n in range(n_clus):\n",
    "                try:           \n",
    "                    count_y.append(int(df_cluster.loc[n]))   \n",
    "                except:\n",
    "                    count_y.append(0)       \n",
    "        else:                   \n",
    "            count_y=list(df_cluster.cluster)   \n",
    "        count.append(count_y)\n",
    "        print (y,end=\" \")\n",
    "    df_count=pd.DataFrame(count, index=year,columns=range(n_clus))\n",
    "    \n",
    "    # 如果有 labels, cluster 替代為 labels\n",
    "    if len(labels)!=n_clus:\n",
    "         labels=list(range(n_clus)) \n",
    "\n",
    "    ## 分群數量，時間趨勢圖 ----------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    year=sorted(set(df.year.unique()))\n",
    "    for c in range(n_clus): \n",
    "        ax.plot(year, df_count[c].rolling(rolling).mean(), label=labels[c])      \n",
    "    ax.set_xticks([1950,1960,1970,1980,1990,2000,2010])\n",
    "\n",
    "    # 圖例位置由程式自己決定\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5)) \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('freq.')    # y lobel\n",
    "    ax.set_title('數量 by year and cluster');   # title\n",
    "    plt.show()\n",
    "\n",
    "    ## 分群比例，時間趨勢圖 -----------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    year=sorted(set(df.year.unique()))\n",
    "    for c in range(n_clus): \n",
    "         ax.plot(year, df_rate[c].rolling(rolling).mean(), label=labels[c])      \n",
    "    ax.set_xticks([1950,1960,1970,1980,1990,2000,2010])\n",
    "\n",
    "    ax.legend(loc=1);  \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('freq.')    # y lobel\n",
    "    ax.set_title('比例 by year and cluster');   # title\n",
    "    plt.show()\n",
    "\n",
    "    ## 各群文章 count 時間疊加圖 -------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    ax.set_xticks([1950,1960,1970,1980,1990,2000,2010])\n",
    "    for n in range(len(labels)):\n",
    "        df_count[n]=df_count[n].rolling(rolling).mean()\n",
    "    if n_clus==2:\n",
    "        plt.stackplot(year, df_count[0],df_count[1], labels=labels)\n",
    "    elif n_clus==3:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2], labels=labels)\n",
    "    elif n_clus==4:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3], labels=labels)        \n",
    "    elif n_clus==5:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4], labels=labels)         \n",
    "    elif n_clus==6:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4],df_count[5], labels=labels)           \n",
    "    elif n_clus==7:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4],df_count[5],df_count[6], labels=labels)         \n",
    "    elif n_clus==8:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4],df_count[5],df_count[6],df_count[7],labels=labels)          \n",
    "    elif n_clus==9:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4],df_count[5],df_count[6],df_count[7],df_count[8],labels=labels)  \n",
    "    elif n_clus==10:\n",
    "        plt.stackplot(year, df_count[0],df_count[1],df_count[2],df_count[3],df_count[4],df_count[5],df_count[6],df_count[7],df_count[8],df_count[9],labels=labels)     \n",
    "\n",
    "    # 圖例位置由程式自己決定\n",
    "    ax.legend(loc=0);  \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('number')    # y lobel\n",
    "    ax.set_title('數量 by year and cluster');   # title\n",
    "    plt.show()\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    ax.set_xticks([1950,1960,1970,1980,2000,2010])\n",
    "\n",
    "    # rate繪疊加圖 \n",
    "    for n in range(len(labels)):\n",
    "        df_rate[n]=df_rate[n].rolling(rolling).mean()\n",
    "                \n",
    "    if n_clus==2:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1], labels=labels)\n",
    "    elif n_clus==3:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2], labels=labels)\n",
    "    elif n_clus==4:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3], labels=labels)        \n",
    "    elif n_clus==5:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4], labels=labels)         \n",
    "    elif n_clus==6:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4],df_rate[5], labels=labels)           \n",
    "    elif n_clus==7:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4],df_rate[5],df_rate[6], labels=labels)         \n",
    "    elif n_clus==8:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4],df_rate[5],df_rate[6],df_rate[7],labels=labels)          \n",
    "    elif n_clus==9:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4],df_rate[5],df_rate[6],df_rate[7],df_rate[8],labels=labels)  \n",
    "    elif n_clus==10:\n",
    "        plt.stackplot(year, df_rate[0],df_rate[1],df_rate[2],df_rate[3],df_rate[4],df_rate[5],df_rate[6],df_rate[7],df_rate[8],df_rate[9],labels=labels)      \n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5)) \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('number')    # y lobel\n",
    "    ax.set_title('比例 by year and cluster');   # title\n",
    "    plt.show()\n",
    "\n",
    "    # 圓餅圖 --------------------------------------------\n",
    "    # percent of each cluster\n",
    "    cluster=list(df.cluster)\n",
    "    percent=[]\n",
    "    for c in range(n_clus): \n",
    "        percent.append(cluster.count(c)/len(cluster))\n",
    "\n",
    "    patches, texts = plt.pie(percent,labels=labels,shadow=False, startangle=90)\n",
    "    #patches, texts = plt.pie(percent,labels=labels,autopct='%1.1f%%',shadow=False, startangle=90)\n",
    "    plt.legend(patches, labels, loc=\"best\")\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 長條圖 --------------------------------------------\n",
    "    # percent of each cluster\n",
    "    total=[]\n",
    "    for c in range(n_clus): \n",
    "        total.append(cluster.count(c))\n",
    "    plt.bar(labels,total)\n",
    "    plt.show()\n",
    "    return df_count, df_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2504b-57ba-456c-9872-33234a11a2f4",
   "metadata": {
    "id": "66d2504b-57ba-456c-9872-33234a11a2f4",
    "tags": []
   },
   "source": [
    "# 潛在語意"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c70e3-d834-4ef6-90fb-bf7518fd9592",
   "metadata": {
    "id": "d71c70e3-d834-4ef6-90fb-bf7518fd9592",
    "tags": []
   },
   "source": [
    "## latent_sementic( df, field, num_i,  pth,title)\n",
    "* 針對 df 內的文本進行潛在語意分析，必須有已斷好詞的欄位。\n",
    "* field 為斷詞後之欄位名稱, 'textrank','tfidf','token' 三者之一。\n",
    "* num_i 主題數目,至多十個類別\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/\n",
    "* 檔案存於 pth+\"data/\"+title+\"_lsa.xlsx\"，如果檔案太大，則目動轉為 pth+\"data/\"+title+\"_df.pkl\"\n",
    "* title 為標題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7cd75bb-383c-4c2c-9216-5be16af30887",
   "metadata": {
    "id": "d7cd75bb-383c-4c2c-9216-5be16af30887",
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_19452/515219167.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_19452/515219167.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def latent_sementic( df, field, num_i, pth=,title):\u001b[0m\n\u001b[1;37m                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def latent_sementic( df, field, num_i, pth=,title):\n",
    "    if field not in ['textrank','tfidf','token']:\n",
    "        print ('field 必是須 textrank,tfidf,token 三者之一')\n",
    "        return\n",
    "    # tfidf 加權\n",
    "    # list of 空白隔開之 string\n",
    "    if field in ['textrank','tfidf']:\n",
    "        df[field]=df[field].str.replace(',',' ')      \n",
    "    \n",
    "    doc_list=df[field].str.replace(\",\",\" \")\n",
    "\n",
    "    # Term document frequency 轉換，成為 matrix ,\n",
    "    transformer = TfidfVectorizer()\n",
    "    tfidf = transformer.fit_transform(doc_list)     # sparse matrix\n",
    "    # 字詞的編號\n",
    "    terms=transformer.vocabulary_\n",
    "    # 字詞的 list\n",
    "    term_list=transformer.get_feature_names_out()\n",
    "    \n",
    "    # latent semantic analysis, 變數縮減\n",
    "    svd = TruncatedSVD(num_i, algorithm = 'arpack')   # object\n",
    "    lsa = svd.fit_transform(tfidf.T)  # numpy array, (number of terms * num_i)     \n",
    "\n",
    "    dtm_lsa = svd.fit_transform(tfidf)  \n",
    "    dtm_com = svd.fit(tfidf)  # object, not numpy array\n",
    "    # components\n",
    "    comp=dtm_com.components_\n",
    "    print(\"維度數\",len(comp))\n",
    "    print(\"字詞數\",len(comp[0]))\n",
    "    \n",
    "    # 每個維度軸都是各字詞的線性組合，每個維度的構成\n",
    "    df_concept=pd.DataFrame(comp,columns= term_list)\n",
    "    df_concept=df_concept.transpose()\n",
    "    # 各維度字詞，依重要性大小排序\n",
    "    con=[]\n",
    "    sig=[]\n",
    "    for n in range(len(df_concept.columns)):\n",
    "        # 由大至小排序\n",
    "        df_order= df_concept.sort_values(by=n, ascending=False)\n",
    "        con.append(list(df_order.index)[0:500])\n",
    "        sig.append(list(df_order[n])[0:500])\n",
    "    # 印出各軸的重要字詞\n",
    "    for n in range(len(con)):\n",
    "        print (n,con[n][:30])    \n",
    "        \n",
    "    # 轉成 dataframe 格式\n",
    "    df_concept=pd.DataFrame(con).T\n",
    "    \n",
    "    # save，必須要有 data 次目錄     \n",
    "    fname=pth+\"data/\"+title+\"_df_concept.xlsx\"\n",
    "    np.save(fname, lsa)\n",
    "\n",
    "    fname=pth+\"data/\"+title+\"_lsa.xlsx\"\n",
    "    writer = pd.ExcelWriter(fname)\n",
    "    df_concept.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "    writer.save()      \n",
    "    \n",
    "    return df_concept, transformer, lsa, term_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd104875-0e82-49ac-be30-97354bee0fd3",
   "metadata": {
    "id": "fd104875-0e82-49ac-be30-97354bee0fd3"
   },
   "source": [
    "## people.similarity(a,b)\n",
    "* 兩向量的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85f49e-8de9-4682-aa41-c37a32b5fbab",
   "metadata": {
    "id": "fd85f49e-8de9-4682-aa41-c37a32b5fbab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def similarity(a,b):\n",
    "    from scipy.spatial import distance\n",
    "    result = 1 - distance.cosine(a, b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0d603-6403-4231-9e5b-231ccac9dc0e",
   "metadata": {
    "id": "96c0d603-6403-4231-9e5b-231ccac9dc0e"
   },
   "source": [
    "## kClosestTerms(k,term,transformer,lsa,term_list)\n",
    "* 針對 lsa_concept 的結果。尋找最接近的K個字詞\n",
    "* k 數字\n",
    "* term 標的詞\n",
    "* transformer 為 lsa_concept 結果變數\n",
    "* lsa 為 lsa_concept 結果變數\n",
    "* term_list 為 lsa_concept 結果變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7f638-a24e-42ff-8945-c00cebbb2b36",
   "metadata": {
    "id": "fce7f638-a24e-42ff-8945-c00cebbb2b36",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kClosestTerms(k,term,transformer,lsa,term_list):     \n",
    "    # 字詞的編號\n",
    "    index = transformer.vocabulary_[term] \n",
    "    # 該字詞與其他字詞的相似度\n",
    "    similar=[]\n",
    "    for i in range(len(lsa)):\n",
    "        s=similarity(lsa[index],lsa[i])\n",
    "        similar.append(s) \n",
    "        if i%10000==0:\n",
    "            print (\"-\",end=\"\")\n",
    "    closestTerms = {}   # dictionary\n",
    "    for i in range(len(lsa)):\n",
    "        closestTerms[term_list[i]] = similar[i]    \n",
    "        if i%10000==0:\n",
    "            print (\"-\",end=\"\")                   \n",
    "    sortedList = sorted(closestTerms , key= lambda l : closestTerms[l])\n",
    "    return sortedList[::-1][0:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066aac9d-aca9-495d-99e3-6fcc9c80e9f0",
   "metadata": {
    "id": "066aac9d-aca9-495d-99e3-6fcc9c80e9f0"
   },
   "source": [
    "# 繪圖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8acfa11-44a4-484d-be4b-6453cf7bbeb6",
   "metadata": {
    "id": "d8acfa11-44a4-484d-be4b-6453cf7bbeb6",
    "tags": []
   },
   "source": [
    "## word_cloud_map(words,title, mask,pth)\n",
    "* 文字雲圖\n",
    "* words 為斷好詞之文本，可以 string, list, dictionary, dataframe （第一欄為字詞，第二欄為頻率） 格式\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/\n",
    "* title 存檔之標題 置於 path+\"plot/\" 檔名 wordcloud_\"+title+\".jpg\"\n",
    "* mask 為底圖，黑白圖片，置於 path+\"pic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e911a-387e-4dd2-8119-268d4cb0091e",
   "metadata": {
    "id": "cd8e911a-387e-4dd2-8119-268d4cb0091e"
   },
   "outputs": [],
   "source": [
    "def word_cloud_map(words,title, mask,pth=\"/content/drive/MyDrive/my python/people/\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from PIL import Image, ImageFont, ImageDraw\n",
    "    from wordcloud import WordCloud, STOPWORDS   \n",
    "    import collections\n",
    "    # win10 要安裝中文字型，請拷貝 simhei.ttf, \n",
    "    #################參數設定\n",
    "    max_font_size=50\n",
    "    min_font_size=3\n",
    "    max_words=500\n",
    "    margin=1\n",
    "    width=1000\n",
    "    height=1000 \n",
    "    BKImg=np.array(Image.open(path+\"pic/\"+mask))\n",
    "    font_path='C:\\Windows\\Fonts\\msjhbd.ttc'\n",
    "    #########################\n",
    "    wordcloud = WordCloud(font_path=font_path,max_font_size=max_font_size, min_font_size = min_font_size\n",
    "                          ,background_color=\"white\", mask = BKImg, margin=margin, height=height, max_words=max_words,width=width)\n",
    "    # 字詞出現的頻率\n",
    "    # 取最前面之1000個字詞繪圖\n",
    "    if type(words)== str:\n",
    "        # wordcloud = wordcloud.generate(string)\n",
    "        word_list=words.split()\n",
    "        counter=collections.Counter(word_list)\n",
    "        most_term=counter.most_common(1000)\n",
    "        wrd_dict=dict(most_term)   \n",
    "    elif type(words)==dict:\n",
    "        wrd_dict=words\n",
    "    elif type(words)==list:\n",
    "        counter=collections.Counter(words)\n",
    "        most_term=counter.most_common(1000)\n",
    "        wrd_dict=dict(most_term)  \n",
    "    elif type(words)==dataframe:    \n",
    "        word=words[words.columns[0]]\n",
    "        freq=words[words.columns[1]]\n",
    "        dict(zip(word, freq))\n",
    "    # 以字詞的頻率 dictionary 繪圖\n",
    "    wordcloud = wordcloud.generate_from_frequencies(wrd_dict)    \n",
    "    print (counter.most_common(20))\n",
    "    # store to file #################################################\n",
    "    wordcloud.to_file(path+\"plot/wordcloud_\"+title+\".jpg\")\n",
    "    # 繪圖\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")      \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be7762-36c8-467c-b436-d01c1b7095b8",
   "metadata": {
    "id": "42be7762-36c8-467c-b436-d01c1b7095b8"
   },
   "source": [
    "## plot_variables_compare(df, variables)\n",
    "* 不同變數間的比較，繪製 盒狀圖、bar、ide 圖\n",
    "* df 必須有 variables 欄位\n",
    "* variables 為 list of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdd1a6-c186-4ba0-b9ed-4fea7878371d",
   "metadata": {
    "id": "87cdd1a6-c186-4ba0-b9ed-4fea7878371d"
   },
   "outputs": [],
   "source": [
    "def plot_variables_compare(df, variables):\n",
    "    vari=[]\n",
    "    for v in variables:\n",
    "        vari.append(df[v])\n",
    "    vari_number=[];vari_mean=[];vari_std=[];vari_name=[];vari_total=[]\n",
    "    for v in range(len(variables)):\n",
    "        vari_name.append(variables[v])\n",
    "        vari_number.append(len(vari[v]))\n",
    "        vari_total.append(sum(vari[v]))\n",
    "        vari_std.append(np.std(vari[v]))\n",
    "        vari_mean.append(np.mean(vari[v]))\n",
    "    data=list(zip(vari_name, vari_number, vari_mean, vari_total, vari_std))\n",
    "    # 變數基本統計 dataframe\n",
    "    df_var=pd.DataFrame(data, columns=[\"name\",\"number\",\"mean\",\"total\",\"std\"])\n",
    "    # 圖面大小\n",
    "    plt.figure(figsize=(8,6))\n",
    "    # 繪製盒狀圖\n",
    "    plt.boxplot(vari,labels=variables)\n",
    "    plt.title(title+'盒狀圖')    # 標題\n",
    "    plt.show()          \n",
    "    return df_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0713a5-e712-461e-85fb-eaec5a7d260c",
   "metadata": {
    "id": "af0713a5-e712-461e-85fb-eaec5a7d260c"
   },
   "source": [
    "## plot_single_variable_cate(df, variable,cate)\n",
    "* 依類別變數分類，比較變數，繪製盒狀圖\n",
    "* df 必須有 variable, cat 欄位\n",
    "* variable 可為 string or list of string\n",
    "* cate 為要區分的類別。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f8312-d88e-40db-99a8-9fee40a192ac",
   "metadata": {
    "id": "938f8312-d88e-40db-99a8-9fee40a192ac"
   },
   "outputs": [],
   "source": [
    "def plot_single_variable_cate(df, variable,cate):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt \n",
    "    from matplotlib import rcParams\n",
    "    rcParams[\"font.sans-serif\"] = [\"kaiu\"] \n",
    "    rcParams[\"font.family\"] =\"DFKai-sb\"\n",
    "    category=sorted(df[cate].unique())\n",
    "    vari=[]\n",
    "    for c in category:\n",
    "        df_cat=df[df[cate]==c]\n",
    "        vari.append(df_cat[variable])\n",
    "    vari_number=[]\n",
    "    vari_mean=[]\n",
    "    vari_std=[]\n",
    "    vari_name=[]\n",
    "    vari_total=[]\n",
    "    for v in range(len(vari)):\n",
    "        vari_name.append(category[v])\n",
    "        vari_number.append(len(vari[v]))\n",
    "        vari_total.append(sum(vari[v]))\n",
    "        vari_std.append(np.std(vari[v]))\n",
    "        vari_mean.append(np.mean(vari[v]))\n",
    "    data=list(zip(vari_name, vari_number, vari_mean, vari_total, vari_std))\n",
    "    df_cat=pd.DataFrame(data, columns=[\"name\",\"number\",\"mean\",\"total\",\"std\"])\n",
    "\n",
    "    # 繪製盒狀圖 --------------------------------------------------------\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.xticks(vari, labels=category) \n",
    "    plt.title(title+'盒狀圖')    # 標題\n",
    "    plt.show()          \n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c4fe8-0758-4020-bac0-06db37625b49",
   "metadata": {
    "id": "281c4fe8-0758-4020-bac0-06db37625b49",
    "tags": []
   },
   "source": [
    "## plot_variables_year_trend(df, variables,rolling)\n",
    "* 變數的時間變化趨勢，繪製 line 圖\n",
    "* df 必須有變數欄位及 year 欄位\n",
    "* variables 為list of string\n",
    "* rolling 為平滑化的單位，取前面n 個數值平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d62709-64f1-41e2-8e01-34e1afee249f",
   "metadata": {
    "id": "85d62709-64f1-41e2-8e01-34e1afee249f"
   },
   "outputs": [],
   "source": [
    "def plot_variables_year_trend(df, variables,rolling=1):  \n",
    "    #繪整至年 \n",
    "    year=[1948,1949,\n",
    "          1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,\n",
    "          1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,\n",
    "          1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,\n",
    "          1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,\n",
    "          1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,\n",
    "          2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,\n",
    "          2010,2011,2012]\n",
    "    if len(df)!= len(year):\n",
    "        vari=[year]\n",
    "        for v in variables:\n",
    "            vari_sub=[]\n",
    "            for y in year:\n",
    "                df_year=df[df.year==y]\n",
    "                if len(df_year)>0:\n",
    "                    vari_sub.append(df_year[v].mean())\n",
    "                else:\n",
    "                    vari_sub.append(0)\n",
    "            vari.append(vari_sub)  \n",
    "        # list transpose\n",
    "        vari=np.array(vari).T.tolist()      \n",
    "        df_mean=pd.DataFrame(vari, index=year,columns=[\"year\"]+variables)\n",
    "    else:\n",
    "        df_mean=df[[\"year\"]+variables].copy()\n",
    "\n",
    "    # line chart ------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    for c in range(len(variables)): \n",
    "        ax.plot(year, df_mean[variables[c]].rolling(rolling).mean(), label=variables[c])\n",
    "    ax.set_xticks([1950,1960,1970,1980,1990,2000,2010])\n",
    "    # 圖例位置由程式自己決定\n",
    "    ax.legend(loc=1);  \n",
    "    ax.set_xlabel('year')     # X label\n",
    "    ax.set_ylabel('value')    # y lobel\n",
    "    # 標題\n",
    "    ax.set_title(title);   # title\n",
    "    plt.show()\n",
    "    return df_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e843d-98fc-411d-a968-89d866e8953d",
   "metadata": {
    "id": "0c4e843d-98fc-411d-a968-89d866e8953d"
   },
   "source": [
    "# 斷詞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c23e0-75d4-4f0c-b671-c56f5e4a8847",
   "metadata": {
    "id": "261c23e0-75d4-4f0c-b671-c56f5e4a8847"
   },
   "source": [
    "## jieba_token( df,field,path, title)\n",
    "* 結巴斷詞，新增 token, ifidf, textrank 欄位，\n",
    "* 必須要有 usdict.txt, deletewords.txt 檔案，置於 path+ \"corpus/\" 下。\n",
    "* field 為未斷詞之df欄位名稱，string 格式\n",
    "* path 內訂資料夾, pth=\"/content/drive/MyDrive/colab/people/\" 或 'D:/my python/people/'\n",
    "* title 標題，字串\n",
    "* 自動儲存結果檔。存於 pth+ \"data\" 下。檔名為 title+\"_jieba.xlsx\"\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617e500-57a5-4a2c-a159-137fffedc854",
   "metadata": {
    "id": "a617e500-57a5-4a2c-a159-137fffedc854"
   },
   "outputs": [],
   "source": [
    "def jieba_token(df,field='doc_content',path=\"/content/drive/MyDrive/colab/people/\", title='doc'):\n",
    "    #encoding=utf-8\n",
    "    # from __future__ import unicode_literals\n",
    "    import sys\n",
    "    import jieba\n",
    "    import jieba.posseg as pseg\n",
    "    import jieba.analyse\n",
    "    import re\n",
    "    import jieba.posseg\n",
    "    from optparse import OptionParser    \n",
    "\n",
    "    ## 結巴斷詞 \n",
    "    print (\"token......\")\n",
    "    # 刪除 doc_content 是 nan 者\n",
    "    df = df.dropna(subset=[field])\n",
    "    df.index=range(len(df))\n",
    "    # 結巴斷詞，先去除雜訊\n",
    "    token=[]\n",
    "    pos=[]\n",
    "    # 呼叫自行定義的詞典(增加詞彙) \n",
    "    jieba.load_userdict( path+ \"corpus/userdic.txt\") \n",
    "    for n in range(len(df)):\n",
    "        s=df[field].iloc[n]  \n",
    "        # 結巴斷詞，斷詞後存成 string，中間空白隔開\n",
    "        res=[]\n",
    "        flag=[]\n",
    "        try:\n",
    "            s=re.sub('[、 ]', '', s)\n",
    "            result = pseg.cut(s)   \n",
    "            for w in result:\n",
    "                if w.word!=\" \":\n",
    "                    res.append(w.word)\n",
    "                    flag.append(w.flag)      \n",
    "            tok=' '.join(res)\n",
    "            po=\" \".join(flag) \n",
    "        except:\n",
    "            tok=\"\"\n",
    "            po=\"\"\n",
    "        token.append(tok) \n",
    "        pos.append(po)\n",
    "        # 計數\n",
    "        if n%1000==0:\n",
    "            print (\"-\",end=\"\")   \n",
    "\n",
    "    df[\"token\"]=token\n",
    "    df[\"pos\"]=pos\n",
    "    content=df[field]\n",
    "\n",
    "    ## tfidf 擷取關鍵字\n",
    "    print (\"tfidf......\")\n",
    "    # 參數設定\n",
    "    topK = 100\n",
    "    # 設停用字\n",
    "    jieba.analyse.set_stop_words(path+\"corpus/deletewords.txt\")\n",
    "    # 自訂辭典\n",
    "    jieba.load_userdict( path+ \"corpus/userdic.txt\")\n",
    "    tfidf=[]\n",
    "    for n in range(len(content)):\n",
    "        try:\n",
    "            tags = jieba.analyse.extract_tags(content[n], topK=topK)\n",
    "            tfidf.append(\",\".join(tags))\n",
    "        except:\n",
    "            tfidf.append(\"\")\n",
    "        if n%1000==0:\n",
    "            print (\"-\",end=\"\")     \n",
    "    df[\"tfidf\"]=tfidf\n",
    "\n",
    "    ## TextRank 擷取關鍵字\n",
    "    print (\"textrank......\")\n",
    "    textrank=[]\n",
    "    for n in range(len(df)):\n",
    "        try:\n",
    "            tags=jieba.analyse.textrank(df[field][n].replace('\\n',''), topK=200, withWeight=False, allowPOS=('a','nt','nz','nr','ns', 'n', 'vn', 'v'))\n",
    "            textrank.append(\",\".join(tags))\n",
    "        except:\n",
    "            textrank.append(\"\")\n",
    "        if n%1000==0:\n",
    "            print (\"-\",end=\"\")        \n",
    "    df[\"textrank\"]=textrank\n",
    "\n",
    "    ## 存檔\n",
    "    fname=str(title)+\"_jieba.xlsx\"\n",
    "    if path.startswith('/content'):\n",
    "        pth=\"/content/drive/MyDrive/my python/people/\"\n",
    "    else:\n",
    "        pth= \"D:/my python/people/\"\n",
    "\n",
    "    df.to_excel( pth+\"data/\" +fname)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec62f0-21c9-4a03-87dd-a614ff29bd58",
   "metadata": {
    "id": "d8ec62f0-21c9-4a03-87dd-a614ff29bd58",
    "tags": []
   },
   "source": [
    "## read_jieba_token(pth, title)\n",
    "* pth=\"/content/drive/MyDrive/my python/people/\" 或 D:/my python/people/\n",
    "* 檔案位於 pth/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cde1ef-b893-4d0c-976d-4ddb410b91be",
   "metadata": {
    "id": "81cde1ef-b893-4d0c-976d-4ddb410b91be"
   },
   "outputs": [],
   "source": [
    "def read_jieba_token( pth=\"/content/drive/MyDrive/my python/people/\", title='doc'):\n",
    "    fname=str(title)+\"_jieba.xlsx\"\n",
    "    df=pd.read_excel(pth+'data/'+fname, 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665f318-135e-4aa3-9dd0-0a9d1758475c",
   "metadata": {
    "id": "0665f318-135e-4aa3-9dd0-0a9d1758475c"
   },
   "source": [
    "# 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828d1b6-12e5-4ffb-b21f-681c34dd9807",
   "metadata": {
    "id": "1828d1b6-12e5-4ffb-b21f-681c34dd9807"
   },
   "source": [
    "## read_file(filename, pth)\n",
    "* 讀取 excel, csv, pkl 檔\n",
    "* pth + filename 為檔案所在的目錄及檔名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b3d3b-6a95-4b55-b82e-8a4532ff13a4",
   "metadata": {
    "id": "116b3d3b-6a95-4b55-b82e-8a4532ff13a4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(filename, pth):\n",
    "    fname=pth+filename\n",
    "    if filename.endswith('.csv'):\n",
    "        df=pd.read_csv(fname, 0)        \n",
    "    elif filename.endswith('.xlsx'):\n",
    "        df=pd.read_excel(fname, 0)    \n",
    "    elif filename.endswith('.pkl'):\n",
    "        df=pd.read_pickle(fname)           \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717c6b2-27f4-4946-a6b3-f1abf0a8eb2a",
   "metadata": {},
   "source": [
    "## save_file(filename, pth)\n",
    "* save excel, csv, pkl 檔\n",
    "* pth + filename 為檔案所在的目錄及檔名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba1a73-6e98-4c75-965c-490f3b9d1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df, filename, pth):\n",
    "    fname=pth+filename\n",
    "    if filename.endswith('.csv'):\n",
    "        df.to_csv(fname)\n",
    "    elif filename.endswith('.xlsx'):\n",
    "        writer = pd.ExcelWriter(fname)\n",
    "        df.to_excel(writer,'Sheet1',encoding='utf-8')\n",
    "        writer.save()\n",
    "    elif filename.endswith('.pkl'):\n",
    "        with open(fname, \"wb\") as fp:  \n",
    "            pickle.dump(df, fp)  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "af0713a5-e712-461e-85fb-eaec5a7d260c",
    "df18138b-bbbe-46c0-a498-8e1f166f6157",
    "40bc549a-c65b-4817-a3db-0f865b59aa66",
    "befb1c0e-98c1-478c-abfc-973789a14456"
   ],
   "name": "people.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
